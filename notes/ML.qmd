---
title: "Machine Learning"
# author: "Yukun Jiao"
date: today
toc: true
editor: 
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

# An Introduction to Statistical Learning (2nd Edition)

**Authors:** Gareth James, Daniela Witten, Trevor Hastie, Robert
Tibshirani\
**Year:** 2021

## Chapter 1

Statistical learning: a vast set of tools for understanding data. These
tools can be classified as supervised or unsupervised.

Regression: predicting a continuous or quantitative output value.

Classification: predicting a categorical or qualitative output.

Clustering: grouping individuals according to their observed
characteristics.

### Notation

-   n --- the number of distinct data points or observations
-   p --- the number of variables that are available for use in making
    predictions
-   $x_{ij}$ --- the value of the $j$th variable for the $i$th
    observation, where $i = 1, 2, \dots, n$ and $j = 1, 2, \dots, p$.
-   $i$ --- observation index
-   $j$ --- variable index
-   $\mathbf{X}$ --- an $n \times p$ matrix whose $(i,j)$th element is
    $x_{ij}$

<!-- $$ -->
<!-- \mathbf{X} = -->
<!-- \begin{pmatrix} -->
<!-- x_{11} & x_{12} & \dots & x_{1p} \\ -->
<!-- x_{21} & x_{22} & \dots & x_{2p} \\ -->
<!-- \vdots & \vdots & \ddots & \vdots \\ -->
<!-- x_{n1} & x_{n2} & \dots & x_{np} -->
<!-- \end{pmatrix}. -->
<!-- $$ -->

<!-- -   $x_i$ --- the row of $\mathbf{X}$ -->

<!-- $$ -->
<!-- x_i = -->
<!-- \begin{pmatrix} -->
<!-- x_{i1}\\ -->
<!-- x_{i2} \\ -->
<!-- \vdots \\ -->
<!-- x_{ip} -->
<!-- \end{pmatrix}. -->
<!-- $$ -->

<!-- -   $x_j$ --- the column of $\mathbf{X}$ -->

<!-- $$ -->
<!-- x_j = -->
<!-- \begin{pmatrix} -->
<!-- x_{1j}\\ -->
<!-- x_{2j} \\ -->
<!-- \vdots \\ -->
<!-- x_{nj} -->
<!-- \end{pmatrix}. -->
<!-- $$ -->

As an example, consider

$$
A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}, \quad
B = \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}.
$$ Then

$$
AB = 
\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}
\begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix}
=
\begin{pmatrix}
1 \times 5 + 2 \times 7 & 1 \times 6 + 2 \times 8 \\
3 \times 5 + 4 \times 7 & 3 \times 6 + 4 \times 8
\end{pmatrix}
=
\begin{pmatrix} 19 & 22 \\ 43 & 50 \end{pmatrix}
$$

## Chapter 2

$X$ --- input variables, predictors, independent variables, features, or
just variables

$Y$ --- response, dependent variable

$Y = f(X) + \epsilon$, where $f$ is some fixed but unknown function of
$X$, and $\epsilon$ is a random error term, which is independent of $X$
and has mean zero (f represents the systematic information that $X$
provides about $Y$.).

statistical learning --- a set of approaches for estimating $f$

### 2.1.1 Why Estimate $f$?

#### Prediction

$$\hat{Y} = \hat{f}(X)$$

$\hat{f}$ --- our estimate for $f$

$\hat{Y}$ --- the resulting prediction for $Y$

The accuracy of $\hat{Y}$ as a prediction of $Y$ depends on two
quantities, reducible error and irreducible error.

reducible error --- We can potentially improve the accuracy of $\hat{f}$
by using the most appropriate statistical learning technique to estimate
$f$.

irreducible error --- introduced by $\epsilon$ ($\hat{Y} = f(X)$,
$Y = f(X) + \epsilon$); this error is lager than zero, because of
unmeasured variables and unmeasurable variation in $\epsilon$.

We have

$$
\begin{aligned}
\mathrm{Var}(X) 
&= E\big[(X - E[X])^2\big] \\
%&= E\big[X^2 - 2X E[X] + (E[X])^2\big] \\
%&= E[X^2] - 2E[X] E[X] + (E[X])^2 \\
&= E[X^2] - (E[X])^2
\end{aligned}
$$

Thus

$$
\begin{aligned}
\mathrm{Var}(\epsilon) 
&= E[\epsilon^2] - 0^2 \\
&= E[\epsilon^2]
\end{aligned}
$$

Then

$$
\begin{aligned}
E(Y - \hat{Y})^2 
&= E\big[ f(X) + \epsilon - \hat{f}(X) \big]^2 \\
&= E\big[ (f(X) - \hat{f}(X)) + \epsilon \big]^2 \\
&= E\big[ (f(X) - \hat{f}(X))^2 + 2(f(X) - \hat{f}(X))\epsilon + \epsilon^2 \big] \\
&= (f(X) - \hat{f}(X))^2 + E(\epsilon^2)\\
&= (f(X) - \hat{f}(X))^2 + \mathrm{Var}(\epsilon)
\end{aligned}
$$

That is

$$
E(Y - \hat{Y})^2
= \underbrace{(f(X) - \hat{f}(X))^2}_{\mathrm{Reducible}}
    + \underbrace{\mathrm{Var}(\epsilon)}_{\mathrm{Irreducible}}
$$

#### Inference

We are often interested in understanding the association between $Y$ and
$X$.

Now $\hat{f}$ cannot be treated as a black box, because we need to know
its exact form. In this setting, one may be interested in answering the
following questions:

-   Which predictors are associated with the response?

-   What is the relationship between the response and each predictor?

-   Can the relationship between $Y$ and each predictor be adequately
    summarized using a linear equation, or is the relationship more
    complicated?

For example,

- Which media are associated with sales?

- Which media generate the biggest boost in sales?

- How large of an increase in sales is associated with a given increase in TV advertising?

An example of modeling for inference --- to what extent is the product’s price associated with sales?

In a real estate setting, 

- How much extra will a house be worth if it has a view of the river? (inference)

- Is this house under- or over-valued? (prediction)

In other words (ChatGPT),

- better understanding the relationship between the response and the predictors -> Inference; 
- accurately predicting the response for future observations -> Prediction.

### 2.1.2 How do we estimate $f$?

#### Parametric Methods

1. We make an assumption about the functional form, or shape, of $f$.
A very simple assumption is that $f$ is linear in $X$ (linear model):

$$
f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
$$

2. After a model has been selected, we need a procedure that uses the training data to fit or train the model. We need to estimate the parameters $\beta_0, \beta_1, \dots, \beta_p$

That is

$$
Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
$$

The most common approach to fitting the model is referred to as ordinary least squares (OLS).
    
#### Non-Parametric Methods

Non-parametric methods do not make explicit assumptions about the functional form of $f$.
Instead they seek an estimate of $f$.

Since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.

- thin-plate spline

More details in Chapter 7

### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability

One question: Why would we ever choose to use a more restrictive method instead of a very flexible approach?

Answer: We might choose a more restrictive method because it is easier to interpret. When our goal is inference, understanding the relationship between each predictor and the response is important, and very flexible methods can produce estimates that are too complex to interpret.

Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. (due to overfitting in highly flexbile methods)

### 2.1.5 Regression Versus Classification Problems

Choice of statistical learning method mainly depends on the type of response:

- Quantitative response → linear regression
- Qualitative response → logistic regression

Predictor type (quantitative or qualitative) is generally less important, as long as any qualitative predictors are properly coded before analysis.


### 2.2.1 Measuring the Quality of Fit

We need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE):

$$
\mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} \big(y_i - \hat{f}(x_i)\big)^2
$$
But this MSE above is training MSE, which we don't care about.

We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.

Thus, we could compute the average squared prediction error for these test observations $(x_0, y_0)$:

$$
\mathrm{Ave}\big( (y_0 - \hat{f}(x_0))^2 \big)
$$

where $(x_0, y_0)$ is a previously unseen test observation not used to train the statistical learning method.

As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.

One important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.


### 2.2.2 The Bias-Variance Trade-Off

For a given value $x_0$, the expected test MSE can always be decomposed into the sum of three fundamental quantities: the variance of $\hat{f}(x_0)$, the squared bias of $\hat{f}(x_0)$, and the variance of the error terms $\epsilon$

That is (bias–variance trade-off)

$$
E\big( y_0 - \hat{f}(x_0) \big)^2 
= \mathrm{Var}\big(\hat{f}(x_0)\big) 
+ \big[ \mathrm{Bias}\big(\hat{f}(x_0)\big) \big]^2 
+ \mathrm{Var}(\epsilon)
$$

This equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias.

variance --- the amount by which $\hat{f}$ would change if we estimated it using a different training data set

In general, more flexible statistical methods have higher variance.

bias --- the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model

In general, more flexible methods result in less bias.

### 2.2.3 The Classification Setting

training error rate (the fraction of incorrect classifications)

$$
\frac{1}{n} \sum_{i=1}^{n} I(y_i \neq \hat{y}_i)
$$
Here, $\hat{y}_i$ is the predicted class label for the $i$th observation using $\hat{f}$. And $I(y_i \neq \hat{y}_i)$ is an indicator variable that equals 1 if $y_i \neq \hat{y}_i$ and 0 if $y_i = \hat{y}_i$. If $I(y_i \neq \hat{y}_i) = 0$, then the $i$th observation was classified correctly by our classification method; otherwise it was misclassified.

test error rate

$$
\mathrm{Ave}\big(I(y_0 \neq \hat{y}_0)\big)
$$


## Chapter 5

Resampling methods: cross-validation, bootstrap

cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.

The bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.

model assessment --- the process of evaluating a model's performance

model selection --- the process of selecting the proper level of flexibility for a model

### The Validation Set Approach

Splitting the set of observations into the training set and the validation set.

Two drawback

- the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.

- the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set (only a subset of the observations, the training set, are used to fit the model---since statistical methods tend to perform worse when trained on fewer observations).

cross-validation --- a refinement of the validation set approach

### Leave-One-Out Cross-Validation

1. validation set --- $(x_1, y_1)$

2. training set --- $\{(x_2, y_2), \dots, (x_n, y_n)\}$

3. The statistical learning method is fit on the n − 1 training observations, repeat n times with a different validation observation.

In other words,

In the $i$-th iteration of LOOCV, the training set contains all observations except the $i$-th one:

$$
\mathrm{Training set}_i = (x_1, y_1), \dots, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \dots, (x_n, y_n)
$$

The validation set consists of the single observation $(x_i, y_i)$. This procedure is repeated for $i = 1, \dots, n$.

Repeating this approach $n$ times produces $n$ squared errors, $\mathrm{MSE}_1, \dots, \mathrm{MSE}_n$. The LOOCV estimate for the test $\mathrm{MSE}$ is the average of these $n$ test error estimates:  

$$
\mathrm{CV}(n) = \frac{1}{n} \sum_{i=1}^{n} \mathrm{MSE}_i
$$
With least squares linear or polynomial regression:

$$
\mathrm{CV}(n) = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2
$$
where $\hat{y}_i$ is the $i$th fitted value from the original least squares fit, and $h_i$ is the leverage defined in (3.37) on page 99.

### k-Fold Cross-Validation

k-fold CV involves randomly dividing the set of observations into k groups, or folds, of approximately equal size.

The $k$-fold CV estimate is computed by averaging these values:

$$
\mathrm{CV}(k) = \frac{1}{k} \sum_{i=1}^{k} \mathrm{MSE}_i.
$$

LOOCV is a special case of k-fold CV in which k is set to equal n.

In practice, one typically performs k-fold CV using k = 5 or k = 10.

The goal here is to identify the method that results in the lowest test error; for this purpose, the location of the minimum point on the estimated test $\mathrm{MSE}$ curve is important, while the actual value of the estimated $\mathrm{MSE}$ is not important.

### Bias-Variance Trade-Off for k-Fold Cross-Validation

LOOCV --- low bias (approximately unbiased estimates) but high variance!

Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.

### Cross-Validation on Classification Problems

In the classification setting, the LOOCV error rate takes the form

$$
\mathrm{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \mathrm{Err}_i
$$
where $\mathrm{Err}_i = I(y_i \neq \hat{y}_i)$

In practice, for real data, the Bayes decision boundary and the test error rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? We can use cross-validation in order to make this decision.

## Chapter 6

How to improve the linear model given that it has distinct advantages in terms of inference and, on real-world problems, is often surprisingly competitive in relation to non-linear methods.

Why? Why might we want to use another fitting procedure instead of least squares?

Because --- alternative fitting procedures can yield better prediction accuracy and model interpretability.

- Prediction Accuracy

- Model Interpretability

three important classes of methods, alternatives to using least squares to fit:

- Subset Selection

- Shrinkage

- Dimension Reduction

### Subset Selection

using a subset of the original variables

omitted

### Shrinkage Methods

shrinking variables' coefficients toward zero

#### Ridge Regression

least squares --- estimates coefficients that minimize

$$
\mathrm{RSS} =  \sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2
$$

Ridge regression --- estimates coefficients that minimize

$$
\mathrm{RSS} + \lambda \sum_{j=1}^p \beta_j^2
$$
where $\lambda \geq 0$ is a tuning parameter.
The second term, called a shrinkage penalty, is small when $\beta_1, \dots, \beta_p$ are close to zero.

ridge regression will produce a different set of coefficient estimates, $\hat{\beta}^{R}_{\lambda}$, for each value of $\lambda$.

Noticing that the shrinkage penalty is applied not to the intercept $\beta_0$.

Taking the derivative of the RSS gives

$$\beta_0 = \bar{y} - \sum_{j=1}^p \beta_j \bar{x}_j$$

assuming that the variables have been centered to have mean zero, that is, $\bar{x}_j = 0$.

Then

$$
\beta_0 = \bar{y} = \sum_{i=1}^n y_i / n
$$

Why Does Ridge Regression Improve Over Least Squares?

Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.

#### The Lasso

Ridge regression's drawback --- Increasing the value of $\lambda$ will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.

The lasso coefficients, $\hat{\beta}_\lambda^L$, minimize the quantity

$$
\text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|
$$

Another Formulation for Ridge Regression and the Lasso

Ridge Regression

$$
\min_{\beta} 
\left\{
\sum_{i=1}^{n} \Big(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\Big)^2
\right\} 
\quad \text{subject to } 
\sum_{j=1}^{p} \beta_j^2 \le s
$$
Lasso

$$
\min_{\beta} 
\left\{
\sum_{i=1}^{n} \Big(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\Big)^2
\right\} 
\quad \text{subject to } 
\sum_{j=1}^{p} |\beta_j| \le s
$$

best subset selection

$$
\min_{\beta} 
\left\{
\sum_{i=1}^{n} \Big(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\Big)^2
\right\} 
\quad \text{subject to } 
\sum_{j=1}^{p} I(\beta_j \neq 0) \le s
$$

This is an exponentially large combinatorial search problem, which is computationally infeasible. In contrast, Ridge regression and Lasso are convex optimization problems.

The Variable Selection Property of the Lasso

constraint regions

$$
|\beta_1| + |\beta_2| \le s \quad \text{and} \quad \beta_1^2 + \beta_2^2 \le s
$$

See Figure 6.7 for details.

Comparing the Lasso and Ridge Regression

In general, one might expect the **lasso** to perform better in a setting where **a relatively small number of predictors have substantial coefficients**, and the remaining predictors have coefficients that are very small or that equal zero. **Ridge regression** will perform better when the response is a function of many predictors, **all with coefficients of roughly equal size**. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as **cross-validation** can be used in order to determine which approach is better on a particular data set.

my question --- why fig 6.9. plots against $R^2$?

A Simple Special Case for Ridge Regression and the Lasso

- ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.

#### Selecting the Tuning Parameter

By cross-validation

### Considerations in High Dimensions

high-dimensional setting --- the case where the number of features $p$ is larger than the number of observations $n$

#### What Goes Wrong in High Dimensions?

The problem is simple: when $p > n$ or $p \approx n$, a simple least squares regression line is too flexible and hence overfits the data.

In high-dimensional settings (when $p \ge n$), traditional methods for adjusting training set error or $R^2$---like $C_p$, $\mathrm{AIC}$, $\mathrm{BIC}$, or adjusted $R^2$---fail because estimating $\hat{\sigma}^2$ becomes unreliable, and adjusted $R^2$ can misleadingly reach 1. Therefore, specialized methods are needed for analyzing high-dimensional data.

#### Regression in High Dimensions

Essentially, these approaches (ridge regression, lasso) avoid overfitting by using a less flexible fitting approach than least squares.

1. regularization or shrinkage plays a key role in high-dimensional problems,
2. appropriate tuning parameter selection is crucial for good predictive performance,
3. the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.

curse of dimensionality --- they can lead to improved predictive models if these features are in fact relevant to the problem at hand, but will lead to worse results if the features are not relevant. Even if they are relevant, the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring.

#### Interpreting Results in High Dimensions

multicollinearity --- the variables in a regression might be correlated with each other

In the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model.

To make it clear that what we have identified is simply one of many possible models for predicting blood pressure, and that it must be further validated on independent data sets.

It is also important to be particularly careful in reporting errors and measures of model fit in the **high-dimensional setting**. We have seen that when $p > n$, it is easy to obtain a useless model that has zero residuals. Therefore, one should never use **sum of squared errors**, **p-values**, **$R^2$ statistics**, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting. 

It is important to instead report results on an **independent test set**, or **cross-validation errors**. For instance, the **MSE** or **R²** on an independent test set is a valid measure of model fit, but the **MSE on the training set** certainly is **not**.

## Chapter 7

### Polynomial Regression

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \cdots + \beta_d x_i^d + \varepsilon_i
$$

### Step Functions

We have

$$
\begin{aligned}
C_0(X) &= I(X < c_1), \\
C_1(X) &= I(c_1 \le X < c_2), \\
C_2(X) &= I(c_2 \le X < c_3), \\
&\;\;\vdots \\
C_{K-1}(X) &= I(c_{K-1} \le X < c_K), \\
C_K(X) &= I(c_K \le X),
\end{aligned}
$$

Then

$$
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \cdots + \beta_K C_K(x_i) + \varepsilon_i.
$$

### Basis Functions

Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach.

The idea is to have at hand a family of functions or transformations that can be applied to a variable $X$: $b_1(X), b_2(X), \dots, b_K(X)$.

Fit the model

$$
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \cdots + \beta_K b_K(x_i) + \varepsilon_i
$$

polynomial regression --- $b_j(x_i) = x_i^j$

step functions / piecewise constant functions --- $b_j(x_i) = I(c_j \le x_i < c_{j+1})$

### Regression Splines

piecewise polynomial + smooth constraint

### Generalized Additive Models


#### GAMs for Regression Problems

$$
\begin{aligned}
y_i &= \beta_0 + \sum_{j=1}^{p} f_j(x_{ij}) + \epsilon_i \\
    &= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \cdots + f_p(x_{ip}) + \epsilon_i
\end{aligned}
$$

#### GAMs for Classification Problems

$$
\log \frac{p(X)}{1 - p(X)} = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p)
$$

#### Pros and Cons of GAMs

### Pros and Cons of GAMs

**Advantages:**

- GAMs allow fitting a non-linear function $f_j$ to each predictor $X_j$, automatically capturing non-linear relationships that standard linear regression might miss. This removes the need to manually try multiple transformations for each variable.

- The non-linear fits can lead to more accurate predictions for the response $Y$.

- The additive nature of the model allows examination of the effect of each $X_j$ on $Y$ individually, while holding other variables fixed.

- The smoothness of each function $f_j$ can be summarized using **degrees of freedom**.

**Limitations:**

- GAMs are restricted to additive effects. With many predictors, important interactions may be missed.

- Interactions can be manually added, either as:
  - Interaction terms $X_j \times X_k$, or  
  - Low-dimensional interaction functions $f_{jk}(X_j, X_k)$, which can be fit using two-dimensional smoothers such as local regression or two-dimensional splines (not covered here).