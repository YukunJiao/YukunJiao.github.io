<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yukun Jiao">
<meta name="dcterms.date" content="2025-08-15">

<title>Machine Learning – Yukun Jiao</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Yukun Jiao</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes/index.html"> 
<span class="menu-text">Note</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lecture-ml-for-social-science" id="toc-lecture-ml-for-social-science" class="nav-link active" data-scroll-target="#lecture-ml-for-social-science">Lecture ML for social science</a>
  <ul class="collapse">
  <li><a href="#lecture-0-and-1" id="toc-lecture-0-and-1" class="nav-link" data-scroll-target="#lecture-0-and-1">Lecture 0 and 1</a></li>
  <li><a href="#lecture-2-and-3" id="toc-lecture-2-and-3" class="nav-link" data-scroll-target="#lecture-2-and-3">Lecture 2 and 3</a></li>
  </ul></li>
  <li><a href="#an-introduction-to-statistical-learning-2nd-edition" id="toc-an-introduction-to-statistical-learning-2nd-edition" class="nav-link" data-scroll-target="#an-introduction-to-statistical-learning-2nd-edition">An Introduction to Statistical Learning (2nd Edition)</a>
  <ul class="collapse">
  <li><a href="#chapter-1" id="toc-chapter-1" class="nav-link" data-scroll-target="#chapter-1">Chapter 1</a></li>
  <li><a href="#chapter-2" id="toc-chapter-2" class="nav-link" data-scroll-target="#chapter-2">Chapter 2</a>
  <ul class="collapse">
  <li><a href="#what-is-statistical-learning" id="toc-what-is-statistical-learning" class="nav-link" data-scroll-target="#what-is-statistical-learning">2.1 What Is Statistical Learning?</a></li>
  <li><a href="#assessing-model-accuracy" id="toc-assessing-model-accuracy" class="nav-link" data-scroll-target="#assessing-model-accuracy">2.2 Assessing Model Accuracy</a></li>
  </ul></li>
  <li><a href="#chapter-5" id="toc-chapter-5" class="nav-link" data-scroll-target="#chapter-5">Chapter 5</a>
  <ul class="collapse">
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation">5.1 Cross-Validation</a></li>
  </ul></li>
  <li><a href="#chapter-6" id="toc-chapter-6" class="nav-link" data-scroll-target="#chapter-6">Chapter 6</a>
  <ul class="collapse">
  <li><a href="#subset-selection" id="toc-subset-selection" class="nav-link" data-scroll-target="#subset-selection">6.1 Subset Selection</a></li>
  <li><a href="#shrinkage-methods" id="toc-shrinkage-methods" class="nav-link" data-scroll-target="#shrinkage-methods">6.2 Shrinkage Methods</a></li>
  <li><a href="#dimension-reduction-methods" id="toc-dimension-reduction-methods" class="nav-link" data-scroll-target="#dimension-reduction-methods">6.3 Dimension Reduction Methods</a></li>
  <li><a href="#considerations-in-high-dimensions" id="toc-considerations-in-high-dimensions" class="nav-link" data-scroll-target="#considerations-in-high-dimensions">6.4 Considerations in High Dimensions</a></li>
  </ul></li>
  <li><a href="#chapter-7" id="toc-chapter-7" class="nav-link" data-scroll-target="#chapter-7">Chapter 7</a>
  <ul class="collapse">
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression">7.1 Polynomial Regression</a></li>
  <li><a href="#step-functions" id="toc-step-functions" class="nav-link" data-scroll-target="#step-functions">7.2 Step Functions</a></li>
  <li><a href="#basis-functions" id="toc-basis-functions" class="nav-link" data-scroll-target="#basis-functions">7.3 Basis Functions</a></li>
  <li><a href="#regression-splines" id="toc-regression-splines" class="nav-link" data-scroll-target="#regression-splines">7.4 Regression Splines</a></li>
  <li><a href="#generalized-additive-models" id="toc-generalized-additive-models" class="nav-link" data-scroll-target="#generalized-additive-models">7.7 Generalized Additive Models</a></li>
  </ul></li>
  <li><a href="#chapter-8" id="toc-chapter-8" class="nav-link" data-scroll-target="#chapter-8">Chapter 8</a>
  <ul class="collapse">
  <li><a href="#the-basics-of-decision-trees" id="toc-the-basics-of-decision-trees" class="nav-link" data-scroll-target="#the-basics-of-decision-trees">8.1 The Basics of Decision Trees</a></li>
  <li><a href="#bagging-random-forests-boosting-and-bayesian-additive-regression-trees" id="toc-bagging-random-forests-boosting-and-bayesian-additive-regression-trees" class="nav-link" data-scroll-target="#bagging-random-forests-boosting-and-bayesian-additive-regression-trees">8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#text-as-data" id="toc-text-as-data" class="nav-link" data-scroll-target="#text-as-data">Text as Data</a>
  <ul class="collapse">
  <li><a href="#chapter-23" id="toc-chapter-23" class="nav-link" data-scroll-target="#chapter-23">Chapter 23</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Machine Learning</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yukun Jiao </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 15, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="lecture-ml-for-social-science" class="level1">
<h1>Lecture ML for social science</h1>
<section id="lecture-0-and-1" class="level2">
<h2 class="anchored" data-anchor-id="lecture-0-and-1">Lecture 0 and 1</h2>
<p><strong>How is ML relevant for social science?</strong></p>
<ul>
<li><p>big data -&gt; human social behavior -&gt; social inquiry</p></li>
<li><p>Digital Trace Data, digitalized content, image data <span class="math inline">\(\dots\)</span></p></li>
</ul>
<p><strong>Course outline</strong></p>
<ul>
<li><p>w1–4: supervised</p></li>
<li><p>w5–7: unsupervised</p></li>
<li><p>w8: ML for causal inference</p></li>
</ul>
<p><strong>two types of supervised learning: prediction, inference</strong></p>
<ul>
<li><p>prediction — an emphasis on <span class="math inline">\(\hat{Y}\)</span></p></li>
<li><p>inference — an emphasis on <span class="math inline">\(\hat{\beta}\)</span></p></li>
</ul>
<p><strong>limitations of standard linear paradigm (OLS)</strong></p>
<ul>
<li><p>Assume linearity</p></li>
<li><p>Assum additivity</p></li>
<li><p>n &gt; p</p></li>
<li><p>Assume we know which variables to include</p></li>
</ul>
<p>To note — standardize oberservations first before regularization</p>
<p><strong>Pros and cons (Ridge and Lasso)</strong></p>
<p><strong>pros</strong>:</p>
<ul>
<li><p>enables OLS</p></li>
<li><p>improve predictability</p></li>
<li><p>imrpove interpretability</p></li>
</ul>
<p><strong>cons</strong>:</p>
<ul>
<li><p>reduce inferential strength</p></li>
<li><p>does not address non-linearity</p></li>
</ul>
<p>splines — forces continuity at knots</p>
<p>additional constraint — linear at the boundaries</p>
</section>
<section id="lecture-2-and-3" class="level2">
<h2 class="anchored" data-anchor-id="lecture-2-and-3">Lecture 2 and 3</h2>
<p>my question: What are the differences between interpretability and explanation</p>
<p>my question: Last month Duncan Watts mentioned two cultures: one is y hat culture emphasizing predictive performance, one is beta hat culture emphasizing causal effects. Both are important but neither alone is sufficient.</p>
<p>my question: how would you integrate prediction with interpretability or explanation, especially as a phd in analytical sociology. I understand that the explanation here could also mean mechanism-based explanation.</p>
<p>(my question: I’m not sure if you were at Duncan Watts’ keynote last month in Norrköping, at IC2S2. If not, I’d just be curious how you think about integrating explanation and prediction，If you were there，then I’m also curious about his ideas on integrating explanation and prediction. How would you comment on his approach? And as a PhD in analytical sociology, how would you integrate explanation—especially mechanism-based explanation—with prediction? 废弃版)</p>
<hr>
</section>
</section>
<section id="an-introduction-to-statistical-learning-2nd-edition" class="level1">
<h1>An Introduction to Statistical Learning (2nd Edition)</h1>
<p><strong>Authors:</strong> Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani<br>
<strong>Year:</strong> 2021</p>
<section id="chapter-1" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1">Chapter 1</h2>
<p>Statistical learning: a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised.</p>
<p>Regression: predicting a continuous or quantitative output value.</p>
<p>Classification: predicting a categorical or qualitative output.</p>
<p>Clustering: grouping individuals according to their observed characteristics.</p>
<section id="notation" class="level4">
<h4 class="anchored" data-anchor-id="notation">Notation</h4>
<ul>
<li>n — the number of distinct data points or observations</li>
<li>p — the number of variables that are available for use in making predictions</li>
<li><span class="math inline">\(x_{ij}\)</span> — the value of the <span class="math inline">\(j\)</span>th variable for the <span class="math inline">\(i\)</span>th observation, where <span class="math inline">\(i = 1, 2, \dots, n\)</span> and <span class="math inline">\(j = 1, 2, \dots, p\)</span>.</li>
<li><span class="math inline">\(i\)</span> — observation index</li>
<li><span class="math inline">\(j\)</span> — variable index</li>
<li><span class="math inline">\(\mathbf{X}\)</span> — an <span class="math inline">\(n \times p\)</span> matrix whose <span class="math inline">\((i,j)\)</span>th element is <span class="math inline">\(x_{ij}\)</span></li>
</ul>
<!-- $$ -->
<!-- \mathbf{X} = -->
<!-- \begin{pmatrix} -->
<!-- x_{11} & x_{12} & \dots & x_{1p} \\ -->
<!-- x_{21} & x_{22} & \dots & x_{2p} \\ -->
<!-- \vdots & \vdots & \ddots & \vdots \\ -->
<!-- x_{n1} & x_{n2} & \dots & x_{np} -->
<!-- \end{pmatrix}. -->
<!-- $$ -->
<!-- -   $x_i$ --- the row of $\mathbf{X}$ -->
<!-- $$ -->
<!-- x_i = -->
<!-- \begin{pmatrix} -->
<!-- x_{i1}\\ -->
<!-- x_{i2} \\ -->
<!-- \vdots \\ -->
<!-- x_{ip} -->
<!-- \end{pmatrix}. -->
<!-- $$ -->
<!-- -   $x_j$ --- the column of $\mathbf{X}$ -->
<!-- $$ -->
<!-- x_j = -->
<!-- \begin{pmatrix} -->
<!-- x_{1j}\\ -->
<!-- x_{2j} \\ -->
<!-- \vdots \\ -->
<!-- x_{nj} -->
<!-- \end{pmatrix}. -->
<!-- $$ -->
<p>As an example, consider</p>
<p><span class="math display">\[
A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}, \quad
B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}.
\]</span> Then</p>
<p><span class="math display">\[
AB =
\begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}
\begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}
=
\begin{pmatrix}
1 \times 5 + 2 \times 7 &amp; 1 \times 6 + 2 \times 8 \\
3 \times 5 + 4 \times 7 &amp; 3 \times 6 + 4 \times 8
\end{pmatrix}
=
\begin{pmatrix} 19 &amp; 22 \\ 43 &amp; 50 \end{pmatrix}
\]</span></p>
</section>
</section>
<section id="chapter-2" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2">Chapter 2</h2>
<p><span class="math inline">\(X\)</span> — input variables, predictors, independent variables, features, or just variables</p>
<p><span class="math inline">\(Y\)</span> — response, dependent variable</p>
<p><span class="math inline">\(Y = f(X) + \epsilon\)</span>, where <span class="math inline">\(f\)</span> is some fixed but unknown function of <span class="math inline">\(X\)</span>, and <span class="math inline">\(\epsilon\)</span> is a random error term, which is independent of <span class="math inline">\(X\)</span> and has mean zero (f represents the systematic information that <span class="math inline">\(X\)</span> provides about <span class="math inline">\(Y\)</span>.).</p>
<p>statistical learning — a set of approaches for estimating <span class="math inline">\(f\)</span></p>
<section id="what-is-statistical-learning" class="level3">
<h3 class="anchored" data-anchor-id="what-is-statistical-learning">2.1 What Is Statistical Learning?</h3>
<section id="why-estimate-f" class="level4">
<h4 class="anchored" data-anchor-id="why-estimate-f">2.1.1 Why Estimate <span class="math inline">\(f\)</span>?</h4>
<section id="prediction" class="level5">
<h5 class="anchored" data-anchor-id="prediction">Prediction</h5>
<p><span class="math display">\[\hat{Y} = \hat{f}(X)\]</span></p>
<p><span class="math inline">\(\hat{f}\)</span> — our estimate for <span class="math inline">\(f\)</span></p>
<p><span class="math inline">\(\hat{Y}\)</span> — the resulting prediction for <span class="math inline">\(Y\)</span></p>
<p>The accuracy of <span class="math inline">\(\hat{Y}\)</span> as a prediction of <span class="math inline">\(Y\)</span> depends on two quantities, reducible error and irreducible error.</p>
<p>reducible error — We can potentially improve the accuracy of <span class="math inline">\(\hat{f}\)</span> by using the most appropriate statistical learning technique to estimate <span class="math inline">\(f\)</span>.</p>
<p>irreducible error — introduced by <span class="math inline">\(\epsilon\)</span> (<span class="math inline">\(\hat{Y} = f(X)\)</span>, <span class="math inline">\(Y = f(X) + \epsilon\)</span>); this error is lager than zero, because of unmeasured variables and unmeasurable variation in <span class="math inline">\(\epsilon\)</span>.</p>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{Var}(X)
&amp;= E\big[(X - E[X])^2\big] \\
%&amp;= E\big[X^2 - 2X E[X] + (E[X])^2\big] \\
%&amp;= E[X^2] - 2E[X] E[X] + (E[X])^2 \\
&amp;= E[X^2] - (E[X])^2
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{Var}(\epsilon)
&amp;= E[\epsilon^2] - 0^2 \\
&amp;= E[\epsilon^2]
\end{aligned}
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\begin{aligned}
E(Y - \hat{Y})^2
&amp;= E\big[ f(X) + \epsilon - \hat{f}(X) \big]^2 \\
&amp;= E\big[ (f(X) - \hat{f}(X)) + \epsilon \big]^2 \\
&amp;= E\big[ (f(X) - \hat{f}(X))^2 + 2(f(X) - \hat{f}(X))\epsilon + \epsilon^2 \big] \\
&amp;= (f(X) - \hat{f}(X))^2 + E(\epsilon^2)\\
&amp;= (f(X) - \hat{f}(X))^2 + \mathrm{Var}(\epsilon)
\end{aligned}
\]</span></p>
<p>That is</p>
<p><span class="math display">\[
E(Y - \hat{Y})^2
= \underbrace{(f(X) - \hat{f}(X))^2}_{\mathrm{Reducible}}
    + \underbrace{\mathrm{Var}(\epsilon)}_{\mathrm{Irreducible}}
\]</span></p>
</section>
<section id="inference" class="level5">
<h5 class="anchored" data-anchor-id="inference">Inference</h5>
<p>We are often interested in understanding the association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.</p>
<p>Now <span class="math inline">\(\hat{f}\)</span> cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:</p>
<ul>
<li><p>Which predictors are associated with the response?</p></li>
<li><p>What is the relationship between the response and each predictor?</p></li>
<li><p>Can the relationship between <span class="math inline">\(Y\)</span> and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?</p></li>
</ul>
<p>For example,</p>
<ul>
<li><p>Which media are associated with sales?</p></li>
<li><p>Which media generate the biggest boost in sales?</p></li>
<li><p>How large of an increase in sales is associated with a given increase in TV advertising?</p></li>
</ul>
<p>An example of modeling for inference — to what extent is the product’s price associated with sales?</p>
<p>In a real estate setting,</p>
<ul>
<li><p>How much extra will a house be worth if it has a view of the river? (inference)</p></li>
<li><p>Is this house under- or over-valued? (prediction)</p></li>
</ul>
<p>In other words (ChatGPT),</p>
<ul>
<li>better understanding the relationship between the response and the predictors -&gt; Inference;</li>
<li>accurately predicting the response for future observations -&gt; Prediction.</li>
</ul>
</section>
</section>
<section id="how-do-we-estimate-f" class="level4">
<h4 class="anchored" data-anchor-id="how-do-we-estimate-f">2.1.2 How do we estimate <span class="math inline">\(f\)</span>?</h4>
<section id="parametric-methods" class="level5">
<h5 class="anchored" data-anchor-id="parametric-methods">Parametric Methods</h5>
<ol type="1">
<li>We make an assumption about the functional form, or shape, of <span class="math inline">\(f\)</span>. A very simple assumption is that <span class="math inline">\(f\)</span> is linear in <span class="math inline">\(X\)</span> (linear model):</li>
</ol>
<p><span class="math display">\[
f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
\]</span></p>
<ol start="2" type="1">
<li>After a model has been selected, we need a procedure that uses the training data to fit or train the model. We need to estimate the parameters <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span></li>
</ol>
<p>That is</p>
<p><span class="math display">\[
Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
\]</span></p>
<p>The most common approach to fitting the model is referred to as ordinary least squares (OLS).</p>
</section>
<section id="non-parametric-methods" class="level5">
<h5 class="anchored" data-anchor-id="non-parametric-methods">Non-Parametric Methods</h5>
<p>Non-parametric methods do not make explicit assumptions about the functional form of <span class="math inline">\(f\)</span>. Instead they seek an estimate of <span class="math inline">\(f\)</span>.</p>
<p>Since they do not reduce the problem of estimating <span class="math inline">\(f\)</span> to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for <span class="math inline">\(f\)</span>.</p>
<ul>
<li>thin-plate spline</li>
</ul>
<p>More details in Chapter 7</p>
</section>
</section>
<section id="the-trade-off-between-prediction-accuracy-and-model-interpretability" class="level4">
<h4 class="anchored" data-anchor-id="the-trade-off-between-prediction-accuracy-and-model-interpretability">2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability</h4>
<p>One question: Why would we ever choose to use a more restrictive method instead of a very flexible approach?</p>
<p>Answer: We might choose a more restrictive method because it is easier to interpret. When our goal is inference, understanding the relationship between each predictor and the response is important, and very flexible methods can produce estimates that are too complex to interpret.</p>
<p>Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. (due to overfitting in highly flexbile methods)</p>
</section>
<section id="regression-versus-classification-problems" class="level4">
<h4 class="anchored" data-anchor-id="regression-versus-classification-problems">2.1.5 Regression Versus Classification Problems</h4>
<p>Choice of statistical learning method mainly depends on the type of response:</p>
<ul>
<li>Quantitative response → linear regression</li>
<li>Qualitative response → logistic regression</li>
</ul>
<p>Predictor type (quantitative or qualitative) is generally less important, as long as any qualitative predictors are properly coded before analysis.</p>
</section>
</section>
<section id="assessing-model-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="assessing-model-accuracy">2.2 Assessing Model Accuracy</h3>
<section id="measuring-the-quality-of-fit" class="level4">
<h4 class="anchored" data-anchor-id="measuring-the-quality-of-fit">2.2.1 Measuring the Quality of Fit</h4>
<p>We need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE):</p>
<p><span class="math display">\[
\mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} \big(y_i - \hat{f}(x_i)\big)^2
\]</span> But this MSE above is training MSE, which we don’t care about.</p>
<p>We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.</p>
<p>Thus, we could compute the average squared prediction error for these test observations <span class="math inline">\((x_0, y_0)\)</span>:</p>
<p><span class="math display">\[
\mathrm{Ave}\big( (y_0 - \hat{f}(x_0))^2 \big)
\]</span></p>
<p>where <span class="math inline">\((x_0, y_0)\)</span> is a previously unseen test observation not used to train the statistical learning method.</p>
<p>As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.</p>
<p>One important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.</p>
</section>
<section id="the-bias-variance-trade-off" class="level4">
<h4 class="anchored" data-anchor-id="the-bias-variance-trade-off">2.2.2 The Bias-Variance Trade-Off</h4>
<p>For a given value <span class="math inline">\(x_0\)</span>, the expected test MSE can always be decomposed into the sum of three fundamental quantities: the variance of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span>, and the variance of the error terms <span class="math inline">\(\epsilon\)</span></p>
<p>That is (bias–variance trade-off)</p>
<p><span class="math display">\[
E\big( y_0 - \hat{f}(x_0) \big)^2
= \mathrm{Var}\big(\hat{f}(x_0)\big)
+ \big[ \mathrm{Bias}\big(\hat{f}(x_0)\big) \big]^2
+ \mathrm{Var}(\epsilon)
\]</span></p>
<p>This equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias.</p>
<p>variance — the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set</p>
<p>In general, more flexible statistical methods have higher variance.</p>
<p>bias — the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model</p>
<p>In general, more flexible methods result in less bias.</p>
</section>
<section id="the-classification-setting" class="level4">
<h4 class="anchored" data-anchor-id="the-classification-setting">2.2.3 The Classification Setting</h4>
<p>training error rate (the fraction of incorrect classifications)</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} I(y_i \neq \hat{y}_i)
\]</span> Here, <span class="math inline">\(\hat{y}_i\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat{f}\)</span>. And <span class="math inline">\(I(y_i \neq \hat{y}_i)\)</span> is an indicator variable that equals 1 if <span class="math inline">\(y_i \neq \hat{y}_i\)</span> and 0 if <span class="math inline">\(y_i = \hat{y}_i\)</span>. If <span class="math inline">\(I(y_i \neq \hat{y}_i) = 0\)</span>, then the <span class="math inline">\(i\)</span>th observation was classified correctly by our classification method; otherwise it was misclassified.</p>
<p>test error rate</p>
<p><span class="math display">\[
\mathrm{Ave}\big(I(y_0 \neq \hat{y}_0)\big)
\]</span></p>
</section>
</section>
</section>
<section id="chapter-5" class="level2">
<h2 class="anchored" data-anchor-id="chapter-5">Chapter 5</h2>
<p>Resampling methods: cross-validation, bootstrap</p>
<p>cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.</p>
<p>The bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.</p>
<p>model assessment — the process of evaluating a model’s performance</p>
<p>model selection — the process of selecting the proper level of flexibility for a model</p>
<section id="cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation">5.1 Cross-Validation</h3>
<section id="the-validation-set-approach" class="level4">
<h4 class="anchored" data-anchor-id="the-validation-set-approach">5.1.1 The Validation Set Approach</h4>
<p>Splitting the set of observations into the training set and the validation set.</p>
<p>Two drawback</p>
<ul>
<li><p>the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li>
<li><p>the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set (only a subset of the observations, the training set, are used to fit the model—since statistical methods tend to perform worse when trained on fewer observations).</p></li>
</ul>
<p>cross-validation — a refinement of the validation set approach</p>
</section>
<section id="leave-one-out-cross-validation" class="level4">
<h4 class="anchored" data-anchor-id="leave-one-out-cross-validation">5.1.2 Leave-One-Out Cross-Validation</h4>
<ol type="1">
<li><p>validation set — <span class="math inline">\((x_1, y_1)\)</span></p></li>
<li><p>training set — <span class="math inline">\(\{(x_2, y_2), \dots, (x_n, y_n)\}\)</span></p></li>
<li><p>The statistical learning method is fit on the n − 1 training observations, repeat n times with a different validation observation.</p></li>
</ol>
<p>In other words,</p>
<p>In the <span class="math inline">\(i\)</span>-th iteration of LOOCV, the training set contains all observations except the <span class="math inline">\(i\)</span>-th one:</p>
<p><span class="math display">\[
\mathrm{Training set}_i = (x_1, y_1), \dots, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \dots, (x_n, y_n)
\]</span></p>
<p>The validation set consists of the single observation <span class="math inline">\((x_i, y_i)\)</span>. This procedure is repeated for <span class="math inline">\(i = 1, \dots, n\)</span>.</p>
<p>Repeating this approach <span class="math inline">\(n\)</span> times produces <span class="math inline">\(n\)</span> squared errors, <span class="math inline">\(\mathrm{MSE}_1, \dots, \mathrm{MSE}_n\)</span>. The LOOCV estimate for the test <span class="math inline">\(\mathrm{MSE}\)</span> is the average of these <span class="math inline">\(n\)</span> test error estimates:</p>
<p><span class="math display">\[
\mathrm{CV}(n) = \frac{1}{n} \sum_{i=1}^{n} \mathrm{MSE}_i
\]</span> With least squares linear or polynomial regression:</p>
<p><span class="math display">\[
\mathrm{CV}(n) = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2
\]</span> where <span class="math inline">\(\hat{y}_i\)</span> is the <span class="math inline">\(i\)</span>th fitted value from the original least squares fit, and <span class="math inline">\(h_i\)</span> is the leverage defined in (3.37) on page 99.</p>
</section>
<section id="k-fold-cross-validation" class="level4">
<h4 class="anchored" data-anchor-id="k-fold-cross-validation">5.1.3 k-Fold Cross-Validation</h4>
<p>k-fold CV involves randomly dividing the set of observations into k groups, or folds, of approximately equal size.</p>
<p>The <span class="math inline">\(k\)</span>-fold CV estimate is computed by averaging these values:</p>
<p><span class="math display">\[
\mathrm{CV}(k) = \frac{1}{k} \sum_{i=1}^{k} \mathrm{MSE}_i.
\]</span></p>
<p>LOOCV is a special case of k-fold CV in which k is set to equal n.</p>
<p>In practice, one typically performs k-fold CV using k = 5 or k = 10.</p>
<p>The goal here is to identify the method that results in the lowest test error; for this purpose, the location of the minimum point on the estimated test <span class="math inline">\(\mathrm{MSE}\)</span> curve is important, while the actual value of the estimated <span class="math inline">\(\mathrm{MSE}\)</span> is not important.</p>
</section>
<section id="bias-variance-trade-off-for-k-fold-cross-validation" class="level4">
<h4 class="anchored" data-anchor-id="bias-variance-trade-off-for-k-fold-cross-validation">5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation</h4>
<p>LOOCV — low bias (approximately unbiased estimates) but high variance!</p>
<p>Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.</p>
</section>
<section id="cross-validation-on-classification-problems" class="level4">
<h4 class="anchored" data-anchor-id="cross-validation-on-classification-problems">5.1.5 Cross-Validation on Classification Problems</h4>
<p>In the classification setting, the LOOCV error rate takes the form</p>
<p><span class="math display">\[
\mathrm{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \mathrm{Err}_i
\]</span> where <span class="math inline">\(\mathrm{Err}_i = I(y_i \neq \hat{y}_i)\)</span></p>
<p>In practice, for real data, the Bayes decision boundary and the test error rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? We can use cross-validation in order to make this decision.</p>
</section>
</section>
</section>
<section id="chapter-6" class="level2">
<h2 class="anchored" data-anchor-id="chapter-6">Chapter 6</h2>
<p>How to improve the linear model given that it has distinct advantages in terms of inference and, on real-world problems, is often surprisingly competitive in relation to non-linear methods.</p>
<p>Why? Why might we want to use another fitting procedure instead of least squares?</p>
<p>Because — alternative fitting procedures can yield better prediction accuracy and model interpretability.</p>
<ul>
<li><p>Prediction Accuracy</p></li>
<li><p>Model Interpretability</p></li>
</ul>
<p>three important classes of methods, alternatives to using least squares to fit:</p>
<ul>
<li><p>Subset Selection</p></li>
<li><p>Shrinkage</p></li>
<li><p>Dimension Reduction</p></li>
</ul>
<section id="subset-selection" class="level3">
<h3 class="anchored" data-anchor-id="subset-selection">6.1 Subset Selection</h3>
<p>using a subset of the original variables</p>
<p>omitted</p>
</section>
<section id="shrinkage-methods" class="level3">
<h3 class="anchored" data-anchor-id="shrinkage-methods">6.2 Shrinkage Methods</h3>
<p>shrinking variables’ coefficients toward zero</p>
<section id="ridge-regression" class="level4">
<h4 class="anchored" data-anchor-id="ridge-regression">6.2.1 Ridge Regression</h4>
<p>least squares — estimates coefficients that minimize</p>
<p><span class="math display">\[
\mathrm{RSS} =  \sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2
\]</span></p>
<p>Ridge regression — estimates coefficients that minimize</p>
<p><span class="math display">\[
\mathrm{RSS} + \lambda \sum_{j=1}^p \beta_j^2
\]</span> where <span class="math inline">\(\lambda \geq 0\)</span> is a tuning parameter. The second term, called a shrinkage penalty, is small when <span class="math inline">\(\beta_1, \dots, \beta_p\)</span> are close to zero.</p>
<p>ridge regression will produce a different set of coefficient estimates, <span class="math inline">\(\hat{\beta}^{R}_{\lambda}\)</span>, for each value of <span class="math inline">\(\lambda\)</span>.</p>
<p>Noticing that the shrinkage penalty is applied not to the intercept <span class="math inline">\(\beta_0\)</span>.</p>
<p>Taking the derivative of the RSS gives</p>
<p><span class="math display">\[\beta_0 = \bar{y} - \sum_{j=1}^p \beta_j \bar{x}_j\]</span></p>
<p>assuming that the variables have been centered to have mean zero, that is, <span class="math inline">\(\bar{x}_j = 0\)</span>.</p>
<p>Then</p>
<p><span class="math display">\[
\beta_0 = \bar{y} = \sum_{i=1}^n y_i / n
\]</span></p>
<p>Why Does Ridge Regression Improve Over Least Squares?</p>
<p>Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.</p>
</section>
<section id="the-lasso" class="level4">
<h4 class="anchored" data-anchor-id="the-lasso">6.2.2 The Lasso</h4>
<p>Ridge regression’s drawback — Increasing the value of <span class="math inline">\(\lambda\)</span> will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.</p>
<p>The lasso coefficients, <span class="math inline">\(\hat{\beta}_\lambda^L\)</span>, minimize the quantity</p>
<p><span class="math display">\[
\text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j|
\]</span></p>
<p>Another Formulation for Ridge Regression and the Lasso</p>
<p>Ridge Regression</p>
<p><span class="math display">\[
\min_{\beta}
\left\{
\sum_{i=1}^{n} \Big(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\Big)^2
\right\}
\quad \text{subject to }
\sum_{j=1}^{p} \beta_j^2 \le s
\]</span> Lasso</p>
<p><span class="math display">\[
\min_{\beta}
\left\{
\sum_{i=1}^{n} \Big(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\Big)^2
\right\}
\quad \text{subject to }
\sum_{j=1}^{p} |\beta_j| \le s
\]</span></p>
<p>best subset selection</p>
<p><span class="math display">\[
\min_{\beta}
\left\{
\sum_{i=1}^{n} \Big(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\Big)^2
\right\}
\quad \text{subject to }
\sum_{j=1}^{p} I(\beta_j \neq 0) \le s
\]</span></p>
<p>This is an exponentially large combinatorial search problem, which is computationally infeasible. In contrast, Ridge regression and Lasso are convex optimization problems.</p>
<p>The Variable Selection Property of the Lasso</p>
<p>constraint regions</p>
<p><span class="math display">\[
|\beta_1| + |\beta_2| \le s \quad \text{and} \quad \beta_1^2 + \beta_2^2 \le s
\]</span></p>
<p>See Figure 6.7 for details.</p>
<p>Comparing the Lasso and Ridge Regression</p>
<p>In general, one might expect the <strong>lasso</strong> to perform better in a setting where <strong>a relatively small number of predictors have substantial coefficients</strong>, and the remaining predictors have coefficients that are very small or that equal zero. <strong>Ridge regression</strong> will perform better when the response is a function of many predictors, <strong>all with coefficients of roughly equal size</strong>. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as <strong>cross-validation</strong> can be used in order to determine which approach is better on a particular data set.</p>
<p>my question — why fig 6.9. plots against <span class="math inline">\(R^2\)</span>?</p>
<p>A Simple Special Case for Ridge Regression and the Lasso</p>
<ul>
<li>ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.</li>
</ul>
</section>
<section id="selecting-the-tuning-parameter" class="level4">
<h4 class="anchored" data-anchor-id="selecting-the-tuning-parameter">6.2.3 Selecting the Tuning Parameter</h4>
<p>By cross-validation</p>
</section>
</section>
<section id="dimension-reduction-methods" class="level3">
<h3 class="anchored" data-anchor-id="dimension-reduction-methods">6.3 Dimension Reduction Methods</h3>
<p>omitted</p>
</section>
<section id="considerations-in-high-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="considerations-in-high-dimensions">6.4 Considerations in High Dimensions</h3>
<section id="high-dimensional-data" class="level4">
<h4 class="anchored" data-anchor-id="high-dimensional-data">6.4.1 High-Dimensional Data</h4>
<p>high-dimensional setting — the case where the number of features <span class="math inline">\(p\)</span> is larger than the number of observations <span class="math inline">\(n\)</span></p>
</section>
<section id="what-goes-wrong-in-high-dimensions" class="level4">
<h4 class="anchored" data-anchor-id="what-goes-wrong-in-high-dimensions">6.4.2 What Goes Wrong in High Dimensions?</h4>
<p>The problem is simple: when <span class="math inline">\(p &gt; n\)</span> or <span class="math inline">\(p \approx n\)</span>, a simple least squares regression line is too flexible and hence overfits the data.</p>
<p>In high-dimensional settings (when <span class="math inline">\(p \ge n\)</span>), traditional methods for adjusting training set error or <span class="math inline">\(R^2\)</span>—like <span class="math inline">\(C_p\)</span>, <span class="math inline">\(\mathrm{AIC}\)</span>, <span class="math inline">\(\mathrm{BIC}\)</span>, or adjusted <span class="math inline">\(R^2\)</span>—fail because estimating <span class="math inline">\(\hat{\sigma}^2\)</span> becomes unreliable, and adjusted <span class="math inline">\(R^2\)</span> can misleadingly reach 1. Therefore, specialized methods are needed for analyzing high-dimensional data.</p>
</section>
<section id="regression-in-high-dimensions" class="level4">
<h4 class="anchored" data-anchor-id="regression-in-high-dimensions">6.4.3 Regression in High Dimensions</h4>
<p>Essentially, these approaches (ridge regression, lasso) avoid overfitting by using a less flexible fitting approach than least squares.</p>
<ol type="1">
<li>regularization or shrinkage plays a key role in high-dimensional problems,</li>
<li>appropriate tuning parameter selection is crucial for good predictive performance,</li>
<li>the test error tends to increase as the dimensionality of the problem (i.e.&nbsp;the number of features or predictors) increases, unless the additional features are truly associated with the response.</li>
</ol>
<p>curse of dimensionality — they can lead to improved predictive models if these features are in fact relevant to the problem at hand, but will lead to worse results if the features are not relevant. Even if they are relevant, the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring.</p>
</section>
<section id="interpreting-results-in-high-dimensions" class="level4">
<h4 class="anchored" data-anchor-id="interpreting-results-in-high-dimensions">6.4.4 Interpreting Results in High Dimensions</h4>
<p>multicollinearity — the variables in a regression might be correlated with each other</p>
<p>In the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model.</p>
<p>To make it clear that what we have identified is simply one of many possible models for predicting blood pressure, and that it must be further validated on independent data sets.</p>
<p>It is also important to be particularly careful in reporting errors and measures of model fit in the <strong>high-dimensional setting</strong>. We have seen that when <span class="math inline">\(p &gt; n\)</span>, it is easy to obtain a useless model that has zero residuals. Therefore, one should never use <strong>sum of squared errors</strong>, <strong>p-values</strong>, <strong><span class="math inline">\(R^2\)</span> statistics</strong>, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting.</p>
<p>It is important to instead report results on an <strong>independent test set</strong>, or <strong>cross-validation errors</strong>. For instance, the <strong>MSE</strong> or <strong>R²</strong> on an independent test set is a valid measure of model fit, but the <strong>MSE on the training set</strong> certainly is <strong>not</strong>.</p>
</section>
</section>
</section>
<section id="chapter-7" class="level2">
<h2 class="anchored" data-anchor-id="chapter-7">Chapter 7</h2>
<section id="polynomial-regression" class="level3">
<h3 class="anchored" data-anchor-id="polynomial-regression">7.1 Polynomial Regression</h3>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \cdots + \beta_d x_i^d + \varepsilon_i
\]</span></p>
</section>
<section id="step-functions" class="level3">
<h3 class="anchored" data-anchor-id="step-functions">7.2 Step Functions</h3>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
C_0(X) &amp;= I(X &lt; c_1), \\
C_1(X) &amp;= I(c_1 \le X &lt; c_2), \\
C_2(X) &amp;= I(c_2 \le X &lt; c_3), \\
&amp;\;\;\vdots \\
C_{K-1}(X) &amp;= I(c_{K-1} \le X &lt; c_K), \\
C_K(X) &amp;= I(c_K \le X),
\end{aligned}
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \cdots + \beta_K C_K(x_i) + \varepsilon_i.
\]</span></p>
</section>
<section id="basis-functions" class="level3">
<h3 class="anchored" data-anchor-id="basis-functions">7.3 Basis Functions</h3>
<p>Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach.</p>
<p>The idea is to have at hand a family of functions or transformations that can be applied to a variable <span class="math inline">\(X\)</span>: <span class="math inline">\(b_1(X), b_2(X), \dots, b_K(X)\)</span>.</p>
<p>Fit the model</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \cdots + \beta_K b_K(x_i) + \varepsilon_i
\]</span></p>
<p>polynomial regression — <span class="math inline">\(b_j(x_i) = x_i^j\)</span></p>
<p>step functions / piecewise constant functions — <span class="math inline">\(b_j(x_i) = I(c_j \le x_i &lt; c_{j+1})\)</span></p>
</section>
<section id="regression-splines" class="level3">
<h3 class="anchored" data-anchor-id="regression-splines">7.4 Regression Splines</h3>
<p>piecewise polynomial + smooth constraint</p>
</section>
<section id="generalized-additive-models" class="level3">
<h3 class="anchored" data-anchor-id="generalized-additive-models">7.7 Generalized Additive Models</h3>
<section id="gams-for-regression-problems" class="level4">
<h4 class="anchored" data-anchor-id="gams-for-regression-problems">GAMs for Regression Problems</h4>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;= \beta_0 + \sum_{j=1}^{p} f_j(x_{ij}) + \epsilon_i \\
    &amp;= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \cdots + f_p(x_{ip}) + \epsilon_i
\end{aligned}
\]</span></p>
</section>
<section id="gams-for-classification-problems" class="level4">
<h4 class="anchored" data-anchor-id="gams-for-classification-problems">GAMs for Classification Problems</h4>
<p><span class="math display">\[
\log \frac{p(X)}{1 - p(X)} = \beta_0 + f_1(X_1) + f_2(X_2) + \cdots + f_p(X_p)
\]</span></p>
</section>
<section id="pros-and-cons-of-gams" class="level4">
<h4 class="anchored" data-anchor-id="pros-and-cons-of-gams">Pros and Cons of GAMs</h4>
<p><strong>Advantages:</strong></p>
<ul>
<li><p>GAMs allow fitting a non-linear function <span class="math inline">\(f_j\)</span> to each predictor <span class="math inline">\(X_j\)</span>, automatically capturing non-linear relationships that standard linear regression might miss. This removes the need to manually try multiple transformations for each variable.</p></li>
<li><p>The non-linear fits can lead to more accurate predictions for the response <span class="math inline">\(Y\)</span>.</p></li>
<li><p>The additive nature of the model allows examination of the effect of each <span class="math inline">\(X_j\)</span> on <span class="math inline">\(Y\)</span> individually, while holding other variables fixed.</p></li>
<li><p>The smoothness of each function <span class="math inline">\(f_j\)</span> can be summarized using <strong>degrees of freedom</strong>.</p></li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><p>GAMs are restricted to additive effects. With many predictors, important interactions may be missed.</p></li>
<li><p>Interactions can be manually added, either as:</p>
<ul>
<li>Interaction terms <span class="math inline">\(X_j \times X_k\)</span>, or<br>
</li>
<li>Low-dimensional interaction functions <span class="math inline">\(f_{jk}(X_j, X_k)\)</span>, which can be fit using two-dimensional smoothers such as local regression or two-dimensional splines (not covered here).</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="chapter-8" class="level2">
<h2 class="anchored" data-anchor-id="chapter-8">Chapter 8</h2>
<p><span class="math display">\[
\text{Predictor space} \xrightarrow{\text{stratifying or segmenting}} \text{Simple regions}
\]</span></p>
<section id="the-basics-of-decision-trees" class="level3">
<h3 class="anchored" data-anchor-id="the-basics-of-decision-trees">8.1 The Basics of Decision Trees</h3>
<section id="regression-trees" class="level4">
<h4 class="anchored" data-anchor-id="regression-trees">8.1.1 Regression Trees</h4>
<p>A simple example</p>
<p>We use the Hitters data set to predict a baseball player’s Salary based on Years and Hits.</p>
<p>Overall, the tree stratifies or segments the players into three regions of predictor space:</p>
<p><span class="math display">\[
R_1 = \{ X \mid \text{Years} &lt; 4.5 \},
R_2 = \{ X \mid \text{Years} \ge 4.5, \ \text{Hits} &lt; 117.5 \},
R_3 = \{ X \mid \text{Years} \ge 4.5, \ \text{Hits} \ge 117.5 \}
\]</span></p>
</section>
<section id="prediction-via-stratification-of-the-feature-space" class="level4">
<h4 class="anchored" data-anchor-id="prediction-via-stratification-of-the-feature-space">Prediction via Stratification of the Feature Space</h4>
<p><strong>How to build a regression tree</strong></p>
<ul>
<li><p>We divide the predictor space—the set of possible values for <span class="math inline">\(X_1, X_2, \dots, X_p\)</span>—into <span class="math inline">\(J\)</span> distinct and non-overlapping regions, <span class="math inline">\(R_1, R_2, \dots, R_J\)</span></p></li>
<li><p>For every observation that falls into the region <span class="math inline">\(R_j\)</span>, we make the same prediction,<br>
which is simply the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span>.</p></li>
</ul>
<p>The goal is to find boxes <span class="math inline">\(R_1, \dots, R_J\)</span> that minimize the RSS, given by</p>
<p><span class="math display">\[
\mathrm{RSS} = \sum_{j=1}^J \sum_{i \in R_j} \bigl(y_i - \hat{y}_{R_j}\bigr)^2
\]</span> where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations within the <span class="math inline">\(j\)</span>th box.</p>
<p>Notes</p>
<ul>
<li><p>this is computationally infeasible</p></li>
<li><p>top-down greedy approach which is recursive binary splitting</p></li>
</ul>
<p><strong>recursive binary splitting</strong></p>
<ul>
<li>selecting the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that spliiting the predicator space into regions <span class="math inline">\(\{X|X_j&lt;s\}\)</span> and <span class="math inline">\(\{X|X_j \ge s\}\)</span> leads to the greatest possible reduction in RSS. That is, we consider all predictors <span class="math inline">\(X_1, \dots, X_p\)</span>, and all possible values of the output <span class="math inline">\(s\)</span> for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS.</li>
</ul>
<p><span class="math display">\[
\sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2,
\]</span></p>
</section>
<section id="tree-pruning" class="level4">
<h4 class="anchored" data-anchor-id="tree-pruning">Tree Pruning</h4>
<p>a better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span>, and then prune it back in order to obtain a subtree.</p>
<p><strong>Cost complexity pruning (weakest link pruning)</strong></p>
<ul>
<li>Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter <span class="math inline">\(\alpha\)</span></li>
</ul>
<p>For each value of α there corresponds a subtree <span class="math inline">\(T \subset T_0\)</span> such that</p>
<p><span class="math display">\[
\sum_{m=1}^{T} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|
\]</span></p>
<p>as we increase <span class="math inline">\(\alpha\)</span> from zero in the above expression, branches get pruned from the tree in a nested and predictable fashion</p>
</section>
<section id="classification-trees" class="level4">
<h4 class="anchored" data-anchor-id="classification-trees">8.1.2 Classification Trees</h4>
<p>For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.</p>
<p><strong>Interpreting the results</strong></p>
<ul>
<li><p>the class prediction corresponding to a particular terminal node region</p></li>
<li><p>the class propotions among the training observations that fall into that region</p></li>
</ul>
<p><strong>Key concept</strong></p>
<ul>
<li><strong>Classification error rate</strong>: A natural alternative to RSS (Residual Sum of Squares) when growing classification trees using recursive binary splitting. However, it is not sufficiently sensitive for tree-growing.</li>
</ul>
<p><span class="math display">\[
E_m = 1 - \max_{k} (\hat{p}_{mk})
\]</span></p>
<ul>
<li><strong>Gini index</strong>: a measure of node purity. The Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are close to zero or one.</li>
</ul>
<p><span class="math display">\[
G = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})
\]</span></p>
<ul>
<li><strong>Entropy</strong>: the entropy will take on a value near zero if the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are all near zero or near one.</li>
</ul>
<p><span class="math display">\[
D = - \sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}
\]</span></p>
<p>Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.</p>
</section>
<section id="trees-versus-linear-models" class="level4">
<h4 class="anchored" data-anchor-id="trees-versus-linear-models">8.1.3 Trees Versus Linear Models</h4>
<p><span class="math display">\[
f(X) = \sum_{m=1}^{M} c_m \cdot 1(X \in R_m)
\]</span></p>
<p><strong>considerations</strong></p>
<ul>
<li><p>the true decision boundary is linear or non-linear.</p></li>
<li><p>test error</p></li>
<li><p>interpretability</p></li>
<li><p>visualization</p></li>
</ul>
</section>
<section id="advantages-and-disadvantages-of-trees" class="level4">
<h4 class="anchored" data-anchor-id="advantages-and-disadvantages-of-trees">8.1.4 Advantages and Disadvantages of Trees</h4>
<p><strong>pros</strong></p>
<ul>
<li><p>Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!</p></li>
<li><p>Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.</p></li>
<li><p>Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).</p></li>
<li><p>Trees can easily handle qualitative predictors without the need to create dummy variables.</p></li>
</ul>
<p><strong>cons</strong></p>
<ul>
<li><p>Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.</p></li>
<li><p>Additionally, trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree.</p></li>
</ul>
<p>However, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved.</p>
</section>
</section>
<section id="bagging-random-forests-boosting-and-bayesian-additive-regression-trees" class="level3">
<h3 class="anchored" data-anchor-id="bagging-random-forests-boosting-and-bayesian-additive-regression-trees">8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees</h3>
<section id="bagging-bootstrap-aggregation" class="level4">
<h4 class="anchored" data-anchor-id="bagging-bootstrap-aggregation">8.2.1 Bagging (Bootstrap aggregation)</h4>
<p>simple idea: averaging a set of observations reduces variance.</p>
<p>to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions.</p>
</section>
<section id="out-of-bag-error-estimation" class="level4">
<h4 class="anchored" data-anchor-id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h4>
<p>可以自然地直接用oob error来估计测试误差 而不需要交叉验证或者验证集</p>
</section>
<section id="variable-importance-measures" class="level4">
<h4 class="anchored" data-anchor-id="variable-importance-measures">Variable Importance Measures</h4>
<p>bagging improves prediction accuracy at the expense of interpretability.</p>
<p>one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees)</p>
<ul>
<li><p>In the case of bagging regression trees，the total amount that the RSS is decreased.</p></li>
<li><p>In the case of bagging classification trees, the total amount that the Gini index is decreased.</p></li>
</ul>
</section>
<section id="random-forests" class="level4">
<h4 class="anchored" data-anchor-id="random-forests">8.2.2 Random Forests</h4>
<p>forcing each split to consider only a subset of the predictors</p>
<p>in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors.</p>
<p>We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.</p>
<p>The main difference between bagging (<span class="math inline">\(m = p\)</span>) and random forests (<span class="math inline">\(m = \sqrt{p}\)</span>) is the choice of predictor subset size <span class="math inline">\(m\)</span>.</p>
<p>The null rate results from simply classifying each observation to the dominant class overall</p>
</section>
<section id="boosting" class="level4">
<h4 class="anchored" data-anchor-id="boosting">Boosting</h4>
<p>Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.</p>
<p>Boosting learns slowly, unlike fitting a single large tree which may overfit.</p>
<p>At each step, a small tree is fitted to the residuals of the current model, not directly to Y.</p>
<p>The new tree is added to the model to update the residuals, gradually improving areas where the model performs poorly.</p>
<p>Tree size is small (controlled by parameter <span class="math inline">\(d\)</span>) and learning speed is further slowed by the shrinkage parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>Unlike bagging, each tree depends on the trees grown before it.</p>
</section>
<section id="section" class="level4">
<h4 class="anchored" data-anchor-id="section">8.2.5</h4>
<p><strong>Summary of Tree Ensemble Methods</strong></p>
<ul>
<li><p>In bagging, the trees are grown independently on random samples of the observations. Consequently, the trees tend to be quite similar to each other. Thus, bagging can get caught in local optima and can fail to thoroughly explore the model space.</p></li>
<li><p>In random forests, the trees are once again grown independently on random samples of the observations. However, each split on each tree is performed using a random subset of the features, thereby decorrelating the trees, and leading to a more thorough exploration of model space relative to bagging.</p></li>
<li><p>In boosting, we only use the original data, and do not draw any random samples. The trees are grown successively, using a “slow” learning approach: each new tree is fit to the signal that is left over from the earlier trees, and shrunken down before it is used.</p></li>
</ul>
<hr>
</section>
</section>
</section>
</section>
<section id="text-as-data" class="level1">
<h1>Text as Data</h1>
<p><strong>Authors:</strong> Brandon M. Stewart</p>
<p><strong>Year:</strong> 2022</p>
<section id="chapter-23" class="level2">
<h2 class="anchored" data-anchor-id="chapter-23">Chapter 23</h2>
<section id="five-principles-of-prediction" class="level4">
<h4 class="anchored" data-anchor-id="five-principles-of-prediction">23.3 Five principles of Prediction</h4>
</section>
<section id="predictive-features-do-not-have-to-cause-the-outcome" class="level4">
<h4 class="anchored" data-anchor-id="predictive-features-do-not-have-to-cause-the-outcome">23.3.1 PREDICTIVE FEATURES DO NOT HAVE TO CAUSE THE OUTCOME</h4>
</section>
<section id="cross-validation-is-not-always-a-good-measure-of-predictive-power" class="level4">
<h4 class="anchored" data-anchor-id="cross-validation-is-not-always-a-good-measure-of-predictive-power">23.3.2 CROSS-VALIDATION IS NOT ALWAYS A GOOD MEASURE OF PREDICTIVE POWER</h4>
</section>
<section id="its-not-always-better-to-be-more-accurate-on-average" class="level4">
<h4 class="anchored" data-anchor-id="its-not-always-better-to-be-more-accurate-on-average">23.3.3 IT’S NOT ALWAYS BETTER TO BE MORE ACCURATE ON AVERAGE</h4>
</section>
<section id="there-can-be-practical-value-in-interpreting-models-for-prediction" class="level4">
<h4 class="anchored" data-anchor-id="there-can-be-practical-value-in-interpreting-models-for-prediction">23.3.4 THERE CAN BE PRACTICAL VALUE IN INTERPRETING MODELS FOR PREDICTION</h4>
</section>
<section id="it-can-be-difficult-to-apply-prediction-to-policymaking" class="level4">
<h4 class="anchored" data-anchor-id="it-can-be-difficult-to-apply-prediction-to-policymaking">23.3.5 IT CAN BE DIFFICULT TO APPLY PREDICTION TO POLICYMAKING</h4>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2025 Yukun Jiao · Built with <a href="https://quarto.org/">Quarto</a> and <a href="https://pages.github.com/">GitHub Pages</a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>