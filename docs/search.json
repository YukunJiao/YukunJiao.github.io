[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Jurassic World and Marx\n\n\n\nmovie\n\n\n\n\n\n\n\n\n\nAug 14, 2025\n\n\nYukun Jiao\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/ML.html",
    "href": "notes/ML.html",
    "title": "Machine Learning",
    "section": "",
    "text": "How is ML relevant for social science?\n\nbig data -&gt; human social behavior -&gt; social inquiry\nDigital Trace Data, digitalized content, image data \\(\\dots\\)\n\nCourse outline\n\nw1–4: supervised\nw5–7: unsupervised\nw8: ML for causal inference\n\ntwo types of supervised learning: prediction, inference\n\nprediction — an emphasis on \\(\\hat{Y}\\)\ninference — an emphasis on \\(\\hat{\\beta}\\)\n\nlimitations of standard linear paradigm (OLS)\n\nAssume linearity\nAssum additivity\nn &gt; p\nAssume we know which variables to include\n\nTo note — standardize oberservations first before regularization\nPros and cons (Ridge and Lasso)\npros:\n\nenables OLS\nimprove predictability\nimrpove interpretability\n\ncons:\n\nreduce inferential strength\ndoes not address non-linearity\n\nsplines — forces continuity at knots\nadditional constraint — linear at the boundaries\n\n\n\nmy question: What are the differences between interpretability and explanation\nmy question: Last month Duncan Watts mentioned two cultures: one is y hat culture emphasizing predictive performance, one is beta hat culture emphasizing causal effects. Both are important but neither alone is sufficient.\nmy question: how would you integrate prediction with interpretability or explanation, especially as a phd in analytical sociology. I understand that the explanation here could also mean mechanism-based explanation.\n(my question: I’m not sure if you were at Duncan Watts’ keynote last month in Norrköping, at IC2S2. If not, I’d just be curious how you think about integrating explanation and prediction，If you were there，then I’m also curious about his ideas on integrating explanation and prediction. How would you comment on his approach? And as a PhD in analytical sociology, how would you integrate explanation—especially mechanism-based explanation—with prediction? 废弃版)"
  },
  {
    "objectID": "notes/ML.html#lab-1",
    "href": "notes/ML.html#lab-1",
    "title": "Machine Learning",
    "section": "",
    "text": "library(ISLR2)\nattach(Wage)"
  },
  {
    "objectID": "notes/ML.html#lecture-0",
    "href": "notes/ML.html#lecture-0",
    "title": "Machine Learning",
    "section": "",
    "text": "How is ML relevant for social science?\n\nbig data -&gt; human social behavior -&gt; social inquiry\nDigital Trace Data, digitalized content, image data \\(\\dots\\)\n\nCourse outline\n\nw1–4: supervised\nw5–7: unsupervised\nw8: ML for causal inference\n\ntwo types of supervised learning: prediction, inference\n\nprediction — an emphasis on \\(\\hat{Y}\\)\ninference — an emphasis on \\(\\hat{\\beta}\\)\n\nlimitations of standard linear paradigm (OLS)\n\nAssume linearity\nAssum additivity\nn &gt; p\nAssume we know which variables to include\n\nTo note — standardize oberservations first before regularization\nPros and cons (Ridge and Lasso)\npros:\n\nenables OLS\nimprove predictability\nimrpove interpretability\n\ncons:\n\nreduce inferential strength\ndoes not address non-linearity\n\nsplines — forces continuity at knots\nadditional constraint — linear at the boundaries"
  },
  {
    "objectID": "notes/ML.html#chapter-1",
    "href": "notes/ML.html#chapter-1",
    "title": "Machine Learning",
    "section": "Chapter 1",
    "text": "Chapter 1\nStatistical learning: a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised.\nRegression: predicting a continuous or quantitative output value.\nClassification: predicting a categorical or qualitative output.\nClustering: grouping individuals according to their observed characteristics.\n\nNotation\n\nn — the number of distinct data points or observations\np — the number of variables that are available for use in making predictions\n\\(x_{ij}\\) — the value of the \\(j\\)th variable for the \\(i\\)th observation, where \\(i = 1, 2, \\dots, n\\) and \\(j = 1, 2, \\dots, p\\).\n\\(i\\) — observation index\n\\(j\\) — variable index\n\\(\\mathbf{X}\\) — an \\(n \\times p\\) matrix whose \\((i,j)\\)th element is \\(x_{ij}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs an example, consider\n\\[\nA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}.\n\\] Then\n\\[\nAB =\n\\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}\n\\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\times 5 + 2 \\times 7 & 1 \\times 6 + 2 \\times 8 \\\\\n3 \\times 5 + 4 \\times 7 & 3 \\times 6 + 4 \\times 8\n\\end{pmatrix}\n=\n\\begin{pmatrix} 19 & 22 \\\\ 43 & 50 \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "notes/ML.html#chapter-2",
    "href": "notes/ML.html#chapter-2",
    "title": "Machine Learning",
    "section": "Chapter 2",
    "text": "Chapter 2\n\\(X\\) — input variables, predictors, independent variables, features, or just variables\n\\(Y\\) — response, dependent variable\n\\(Y = f(X) + \\epsilon\\), where \\(f\\) is some fixed but unknown function of \\(X\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero (f represents the systematic information that \\(X\\) provides about \\(Y\\).).\nstatistical learning — a set of approaches for estimating \\(f\\)\n\n2.1 What Is Statistical Learning?\n\n2.1.1 Why Estimate \\(f\\)?\n\nPrediction\n\\[\\hat{Y} = \\hat{f}(X)\\]\n\\(\\hat{f}\\) — our estimate for \\(f\\)\n\\(\\hat{Y}\\) — the resulting prediction for \\(Y\\)\nThe accuracy of \\(\\hat{Y}\\) as a prediction of \\(Y\\) depends on two quantities, reducible error and irreducible error.\nreducible error — We can potentially improve the accuracy of \\(\\hat{f}\\) by using the most appropriate statistical learning technique to estimate \\(f\\).\nirreducible error — introduced by \\(\\epsilon\\) (\\(\\hat{Y} = f(X)\\), \\(Y = f(X) + \\epsilon\\)); this error is lager than zero, because of unmeasured variables and unmeasurable variation in \\(\\epsilon\\).\nWe have\n\\[\n\\begin{aligned}\n\\mathrm{Var}(X)\n&= E\\big[(X - E[X])^2\\big] \\\\\n%&= E\\big[X^2 - 2X E[X] + (E[X])^2\\big] \\\\\n%&= E[X^2] - 2E[X] E[X] + (E[X])^2 \\\\\n&= E[X^2] - (E[X])^2\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\n\\mathrm{Var}(\\epsilon)\n&= E[\\epsilon^2] - 0^2 \\\\\n&= E[\\epsilon^2]\n\\end{aligned}\n\\]\nThen\n\\[\n\\begin{aligned}\nE(Y - \\hat{Y})^2\n&= E\\big[ f(X) + \\epsilon - \\hat{f}(X) \\big]^2 \\\\\n&= E\\big[ (f(X) - \\hat{f}(X)) + \\epsilon \\big]^2 \\\\\n&= E\\big[ (f(X) - \\hat{f}(X))^2 + 2(f(X) - \\hat{f}(X))\\epsilon + \\epsilon^2 \\big] \\\\\n&= (f(X) - \\hat{f}(X))^2 + E(\\epsilon^2)\\\\\n&= (f(X) - \\hat{f}(X))^2 + \\mathrm{Var}(\\epsilon)\n\\end{aligned}\n\\]\nThat is\n\\[\nE(Y - \\hat{Y})^2\n= \\underbrace{(f(X) - \\hat{f}(X))^2}_{\\mathrm{Reducible}}\n    + \\underbrace{\\mathrm{Var}(\\epsilon)}_{\\mathrm{Irreducible}}\n\\]\n\n\nInference\nWe are often interested in understanding the association between \\(Y\\) and \\(X\\).\nNow \\(\\hat{f}\\) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:\n\nWhich predictors are associated with the response?\nWhat is the relationship between the response and each predictor?\nCan the relationship between \\(Y\\) and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\nFor example,\n\nWhich media are associated with sales?\nWhich media generate the biggest boost in sales?\nHow large of an increase in sales is associated with a given increase in TV advertising?\n\nAn example of modeling for inference — to what extent is the product’s price associated with sales?\nIn a real estate setting,\n\nHow much extra will a house be worth if it has a view of the river? (inference)\nIs this house under- or over-valued? (prediction)\n\nIn other words (ChatGPT),\n\nbetter understanding the relationship between the response and the predictors -&gt; Inference;\naccurately predicting the response for future observations -&gt; Prediction.\n\n\n\n\n2.1.2 How do we estimate \\(f\\)?\n\nParametric Methods\n\nWe make an assumption about the functional form, or shape, of \\(f\\). A very simple assumption is that \\(f\\) is linear in \\(X\\) (linear model):\n\n\\[\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\n\\]\n\nAfter a model has been selected, we need a procedure that uses the training data to fit or train the model. We need to estimate the parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)\n\nThat is\n\\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\n\\]\nThe most common approach to fitting the model is referred to as ordinary least squares (OLS).\n\n\nNon-Parametric Methods\nNon-parametric methods do not make explicit assumptions about the functional form of \\(f\\). Instead they seek an estimate of \\(f\\).\nSince they do not reduce the problem of estimating \\(f\\) to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for \\(f\\).\n\nthin-plate spline\n\nMore details in Chapter 7\n\n\n\n2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability\nOne question: Why would we ever choose to use a more restrictive method instead of a very flexible approach?\nAnswer: We might choose a more restrictive method because it is easier to interpret. When our goal is inference, understanding the relationship between each predictor and the response is important, and very flexible methods can produce estimates that are too complex to interpret.\nSurprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. (due to overfitting in highly flexbile methods)\n\n\n2.1.5 Regression Versus Classification Problems\nChoice of statistical learning method mainly depends on the type of response:\n\nQuantitative response → linear regression\nQualitative response → logistic regression\n\nPredictor type (quantitative or qualitative) is generally less important, as long as any qualitative predictors are properly coded before analysis.\n\n\n\n2.2 Assessing Model Accuracy\n\n2.2.1 Measuring the Quality of Fit\nWe need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE):\n\\[\n\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_i - \\hat{f}(x_i)\\big)^2\n\\] But this MSE above is training MSE, which we don’t care about.\nWe are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.\nThus, we could compute the average squared prediction error for these test observations \\((x_0, y_0)\\):\n\\[\n\\mathrm{Ave}\\big( (y_0 - \\hat{f}(x_0))^2 \\big)\n\\]\nwhere \\((x_0, y_0)\\) is a previously unseen test observation not used to train the statistical learning method.\nAs model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.\nOne important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.\n\n\n2.2.2 The Bias-Variance Trade-Off\nFor a given value \\(x_0\\), the expected test MSE can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\), and the variance of the error terms \\(\\epsilon\\)\nThat is (bias–variance trade-off)\n\\[\nE\\big( y_0 - \\hat{f}(x_0) \\big)^2\n= \\mathrm{Var}\\big(\\hat{f}(x_0)\\big)\n+ \\big[ \\mathrm{Bias}\\big(\\hat{f}(x_0)\\big) \\big]^2\n+ \\mathrm{Var}(\\epsilon)\n\\]\nThis equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias.\nvariance — the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set\nIn general, more flexible statistical methods have higher variance.\nbias — the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model\nIn general, more flexible methods result in less bias.\n\n\n2.2.3 The Classification Setting\ntraining error rate (the fraction of incorrect classifications)\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)\n\\] Here, \\(\\hat{y}_i\\) is the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\). And \\(I(y_i \\neq \\hat{y}_i)\\) is an indicator variable that equals 1 if \\(y_i \\neq \\hat{y}_i\\) and 0 if \\(y_i = \\hat{y}_i\\). If \\(I(y_i \\neq \\hat{y}_i) = 0\\), then the \\(i\\)th observation was classified correctly by our classification method; otherwise it was misclassified.\ntest error rate\n\\[\n\\mathrm{Ave}\\big(I(y_0 \\neq \\hat{y}_0)\\big)\n\\]"
  },
  {
    "objectID": "notes/ML.html#chapter-5",
    "href": "notes/ML.html#chapter-5",
    "title": "Machine Learning",
    "section": "Chapter 5",
    "text": "Chapter 5\nResampling methods: cross-validation, bootstrap\ncross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.\nThe bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.\nmodel assessment — the process of evaluating a model’s performance\nmodel selection — the process of selecting the proper level of flexibility for a model\n\n5.1 Cross-Validation\n\n5.1.1 The Validation Set Approach\nSplitting the set of observations into the training set and the validation set.\nTwo drawback\n\nthe validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nthe validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set (only a subset of the observations, the training set, are used to fit the model—since statistical methods tend to perform worse when trained on fewer observations).\n\ncross-validation — a refinement of the validation set approach\n\n\n5.1.2 Leave-One-Out Cross-Validation\n\nvalidation set — \\((x_1, y_1)\\)\ntraining set — \\(\\{(x_2, y_2), \\dots, (x_n, y_n)\\}\\)\nThe statistical learning method is fit on the n − 1 training observations, repeat n times with a different validation observation.\n\nIn other words,\nIn the \\(i\\)-th iteration of LOOCV, the training set contains all observations except the \\(i\\)-th one:\n\\[\n\\mathrm{Training set}_i = (x_1, y_1), \\dots, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \\dots, (x_n, y_n)\n\\]\nThe validation set consists of the single observation \\((x_i, y_i)\\). This procedure is repeated for \\(i = 1, \\dots, n\\).\nRepeating this approach \\(n\\) times produces \\(n\\) squared errors, \\(\\mathrm{MSE}_1, \\dots, \\mathrm{MSE}_n\\). The LOOCV estimate for the test \\(\\mathrm{MSE}\\) is the average of these \\(n\\) test error estimates:\n\\[\n\\mathrm{CV}(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{MSE}_i\n\\] With least squares linear or polynomial regression:\n\\[\n\\mathrm{CV}(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\n\\] where \\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the original least squares fit, and \\(h_i\\) is the leverage defined in (3.37) on page 99.\n\n\n5.1.3 k-Fold Cross-Validation\nk-fold CV involves randomly dividing the set of observations into k groups, or folds, of approximately equal size.\nThe \\(k\\)-fold CV estimate is computed by averaging these values:\n\\[\n\\mathrm{CV}(k) = \\frac{1}{k} \\sum_{i=1}^{k} \\mathrm{MSE}_i.\n\\]\nLOOCV is a special case of k-fold CV in which k is set to equal n.\nIn practice, one typically performs k-fold CV using k = 5 or k = 10.\nThe goal here is to identify the method that results in the lowest test error; for this purpose, the location of the minimum point on the estimated test \\(\\mathrm{MSE}\\) curve is important, while the actual value of the estimated \\(\\mathrm{MSE}\\) is not important.\n\n\n5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation\nLOOCV — low bias (approximately unbiased estimates) but high variance!\nSince the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.\n\n\n5.1.5 Cross-Validation on Classification Problems\nIn the classification setting, the LOOCV error rate takes the form\n\\[\n\\mathrm{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{Err}_i\n\\] where \\(\\mathrm{Err}_i = I(y_i \\neq \\hat{y}_i)\\)\nIn practice, for real data, the Bayes decision boundary and the test error rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? We can use cross-validation in order to make this decision."
  },
  {
    "objectID": "notes/ML.html#chapter-6",
    "href": "notes/ML.html#chapter-6",
    "title": "Machine Learning",
    "section": "Chapter 6",
    "text": "Chapter 6\nHow to improve the linear model given that it has distinct advantages in terms of inference and, on real-world problems, is often surprisingly competitive in relation to non-linear methods.\nWhy? Why might we want to use another fitting procedure instead of least squares?\nBecause — alternative fitting procedures can yield better prediction accuracy and model interpretability.\n\nPrediction Accuracy\nModel Interpretability\n\nthree important classes of methods, alternatives to using least squares to fit:\n\nSubset Selection\nShrinkage\nDimension Reduction\n\n\n6.1 Subset Selection\nusing a subset of the original variables\nomitted\n\n\n6.2 Shrinkage Methods\nshrinking variables’ coefficients toward zero\n\n6.2.1 Ridge Regression\nleast squares — estimates coefficients that minimize\n\\[\n\\mathrm{RSS} =  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2\n\\]\nRidge regression — estimates coefficients that minimize\n\\[\n\\mathrm{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] where \\(\\lambda \\geq 0\\) is a tuning parameter. The second term, called a shrinkage penalty, is small when \\(\\beta_1, \\dots, \\beta_p\\) are close to zero.\nridge regression will produce a different set of coefficient estimates, \\(\\hat{\\beta}^{R}_{\\lambda}\\), for each value of \\(\\lambda\\).\nNoticing that the shrinkage penalty is applied not to the intercept \\(\\beta_0\\).\nTaking the derivative of the RSS gives\n\\[\\beta_0 = \\bar{y} - \\sum_{j=1}^p \\beta_j \\bar{x}_j\\]\nassuming that the variables have been centered to have mean zero, that is, \\(\\bar{x}_j = 0\\).\nThen\n\\[\n\\beta_0 = \\bar{y} = \\sum_{i=1}^n y_i / n\n\\]\nWhy Does Ridge Regression Improve Over Least Squares?\nRidge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n\n\n6.2.2 The Lasso\nRidge regression’s drawback — Increasing the value of \\(\\lambda\\) will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.\nThe lasso coefficients, \\(\\hat{\\beta}_\\lambda^L\\), minimize the quantity\n\\[\n\\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\nAnother Formulation for Ridge Regression and the Lasso\nRidge Regression\n\\[\n\\min_{\\beta}\n\\left\\{\n\\sum_{i=1}^{n} \\Big(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\Big)^2\n\\right\\}\n\\quad \\text{subject to }\n\\sum_{j=1}^{p} \\beta_j^2 \\le s\n\\] Lasso\n\\[\n\\min_{\\beta}\n\\left\\{\n\\sum_{i=1}^{n} \\Big(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\Big)^2\n\\right\\}\n\\quad \\text{subject to }\n\\sum_{j=1}^{p} |\\beta_j| \\le s\n\\]\nbest subset selection\n\\[\n\\min_{\\beta}\n\\left\\{\n\\sum_{i=1}^{n} \\Big(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\Big)^2\n\\right\\}\n\\quad \\text{subject to }\n\\sum_{j=1}^{p} I(\\beta_j \\neq 0) \\le s\n\\]\nThis is an exponentially large combinatorial search problem, which is computationally infeasible. In contrast, Ridge regression and Lasso are convex optimization problems.\nThe Variable Selection Property of the Lasso\nconstraint regions\n\\[\n|\\beta_1| + |\\beta_2| \\le s \\quad \\text{and} \\quad \\beta_1^2 + \\beta_2^2 \\le s\n\\]\nSee Figure 6.7 for details.\nComparing the Lasso and Ridge Regression\nIn general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.\nmy question — why fig 6.9. plots against \\(R^2\\)?\nA Simple Special Case for Ridge Regression and the Lasso\n\nridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.\n\n\n\n6.2.3 Selecting the Tuning Parameter\nBy cross-validation\n\n\n\n6.3 Dimension Reduction Methods\nomitted\n\n\n6.4 Considerations in High Dimensions\n\n6.4.1 High-Dimensional Data\nhigh-dimensional setting — the case where the number of features \\(p\\) is larger than the number of observations \\(n\\)\n\n\n6.4.2 What Goes Wrong in High Dimensions?\nThe problem is simple: when \\(p &gt; n\\) or \\(p \\approx n\\), a simple least squares regression line is too flexible and hence overfits the data.\nIn high-dimensional settings (when \\(p \\ge n\\)), traditional methods for adjusting training set error or \\(R^2\\)—like \\(C_p\\), \\(\\mathrm{AIC}\\), \\(\\mathrm{BIC}\\), or adjusted \\(R^2\\)—fail because estimating \\(\\hat{\\sigma}^2\\) becomes unreliable, and adjusted \\(R^2\\) can misleadingly reach 1. Therefore, specialized methods are needed for analyzing high-dimensional data.\n\n\n6.4.3 Regression in High Dimensions\nEssentially, these approaches (ridge regression, lasso) avoid overfitting by using a less flexible fitting approach than least squares.\n\nregularization or shrinkage plays a key role in high-dimensional problems,\nappropriate tuning parameter selection is crucial for good predictive performance,\nthe test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.\n\ncurse of dimensionality — they can lead to improved predictive models if these features are in fact relevant to the problem at hand, but will lead to worse results if the features are not relevant. Even if they are relevant, the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring.\n\n\n6.4.4 Interpreting Results in High Dimensions\nmulticollinearity — the variables in a regression might be correlated with each other\nIn the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model.\nTo make it clear that what we have identified is simply one of many possible models for predicting blood pressure, and that it must be further validated on independent data sets.\nIt is also important to be particularly careful in reporting errors and measures of model fit in the high-dimensional setting. We have seen that when \\(p &gt; n\\), it is easy to obtain a useless model that has zero residuals. Therefore, one should never use sum of squared errors, p-values, \\(R^2\\) statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting.\nIt is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R² on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not."
  },
  {
    "objectID": "notes/ML.html#chapter-7",
    "href": "notes/ML.html#chapter-7",
    "title": "Machine Learning",
    "section": "Chapter 7",
    "text": "Chapter 7\n\n7.1 Polynomial Regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\cdots + \\beta_d x_i^d + \\varepsilon_i\n\\]\n\n\n7.2 Step Functions\nWe have\n\\[\n\\begin{aligned}\nC_0(X) &= I(X &lt; c_1), \\\\\nC_1(X) &= I(c_1 \\le X &lt; c_2), \\\\\nC_2(X) &= I(c_2 \\le X &lt; c_3), \\\\\n&\\;\\;\\vdots \\\\\nC_{K-1}(X) &= I(c_{K-1} \\le X &lt; c_K), \\\\\nC_K(X) &= I(c_K \\le X),\n\\end{aligned}\n\\]\nThen\n\\[\ny_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i) + \\cdots + \\beta_K C_K(x_i) + \\varepsilon_i.\n\\]\n\n\n7.3 Basis Functions\nPolynomial and piecewise-constant regression models are in fact special cases of a basis function approach.\nThe idea is to have at hand a family of functions or transformations that can be applied to a variable \\(X\\): \\(b_1(X), b_2(X), \\dots, b_K(X)\\).\nFit the model\n\\[\ny_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\beta_3 b_3(x_i) + \\cdots + \\beta_K b_K(x_i) + \\varepsilon_i\n\\]\npolynomial regression — \\(b_j(x_i) = x_i^j\\)\nstep functions / piecewise constant functions — \\(b_j(x_i) = I(c_j \\le x_i &lt; c_{j+1})\\)\n\n\n7.4 Regression Splines\npiecewise polynomial + smooth constraint\n\n\n7.7 Generalized Additive Models\n\nGAMs for Regression Problems\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\sum_{j=1}^{p} f_j(x_{ij}) + \\epsilon_i \\\\\n    &= \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\cdots + f_p(x_{ip}) + \\epsilon_i\n\\end{aligned}\n\\]\n\n\nGAMs for Classification Problems\n\\[\n\\log \\frac{p(X)}{1 - p(X)} = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p)\n\\]\n\n\nPros and Cons of GAMs\nAdvantages:\n\nGAMs allow fitting a non-linear function \\(f_j\\) to each predictor \\(X_j\\), automatically capturing non-linear relationships that standard linear regression might miss. This removes the need to manually try multiple transformations for each variable.\nThe non-linear fits can lead to more accurate predictions for the response \\(Y\\).\nThe additive nature of the model allows examination of the effect of each \\(X_j\\) on \\(Y\\) individually, while holding other variables fixed.\nThe smoothness of each function \\(f_j\\) can be summarized using degrees of freedom.\n\nLimitations:\n\nGAMs are restricted to additive effects. With many predictors, important interactions may be missed.\nInteractions can be manually added, either as:\n\nInteraction terms \\(X_j \\times X_k\\), or\n\nLow-dimensional interaction functions \\(f_{jk}(X_j, X_k)\\), which can be fit using two-dimensional smoothers such as local regression or two-dimensional splines (not covered here)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yukun Jiao",
    "section": "",
    "text": "Yukun Jiao is a Master’s student in Computational Social Science at Linköping University.\nWhen not being a nerd over computational social science, he enjoys pretending to work out and watching ducks, pigeons, and seagulls. Yukun was born and raised in North China."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Yukun Jiao",
    "section": "",
    "text": "Yukun Jiao is a Master’s student in Computational Social Science at Linköping University.\nWhen not being a nerd over computational social science, he enjoys pretending to work out and watching ducks, pigeons, and seagulls. Yukun was born and raised in North China."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Yukun Jiao",
    "section": "Research Interests",
    "text": "Research Interests\n\nCausal Inference\nSocial Network Analysis\nComputational Text Analysis"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Yukun Jiao",
    "section": "Projects",
    "text": "Projects\n\nResearch Internship in THE COMPLETE NETWORK OF SWEDEN"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Yukun Jiao",
    "section": "Publications",
    "text": "Publications\n\nNone yet ¯\\_(ツ)_/¯"
  },
  {
    "objectID": "index.html#activities",
    "href": "index.html#activities",
    "title": "Yukun Jiao",
    "section": "Activities",
    "text": "Activities\n\nVolunteered at SICSS-IAS 2025\nSneaked into Sunbelt 2025 — unregistered but had fun\nVolunteered at IC2S2 2025"
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Yukun Jiao",
    "section": "Contact me",
    "text": "Contact me\n\nEmail: yukji739@student.liu.se \nInstagram: yukunjiao233"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "My Notes",
    "section": "",
    "text": "Click the links below to view the detailed notes:\n\nMachine Learning\nHypergraph"
  },
  {
    "objectID": "notes/ML_lab.html",
    "href": "notes/ML_lab.html",
    "title": "Machine Learning Lab",
    "section": "",
    "text": "library(data.table)\nlibrary(glmnet)\nlibrary(ggplot2)\nlibrary(mlbench)\nlibrary(caret)\nlibrary(splines)\nlibrary(ggeffects)\n\n\n# load data\ntrumpbernie &lt;- fread(file = \"~/Documents/ML/Labs/Lab1/trumpbernie.csv\")\n\n# inspect\n# trumpbernie[1:10,1:10]\ndim(trumpbernie) \n\n[1] 1003 1496\n\n\nThe dataset is 1003-row and 1496-column one. The number of variables (1496) &gt; the number of observation (1003), so this dataset is high-dimensional. Based on this, a standard logistic regression won’t work well for the purpose of prediction due to overfitting and multilinearity.\n\n\n\n\nmymodel &lt;- glm(trump_tweet ~ ., data = trumpbernie, family=\"binomial\")\n\nWarning: glm.fit: algorithm did not converge\n\nsum(!is.na(coef(mymodel)))\n\n[1] 1001\n\ncoef(mymodel)[(1001:1050)]\n\n     popul    popular    possibl       post    potenti    poverti      power \n 349.33967 -771.91767  -30.26994   25.75710 -380.91299   59.38732         NA \n   practic     prayer     prefer    premium     prepar       pres  prescript \n        NA         NA         NA         NA         NA         NA         NA \n   present     presid presidenti      press    pressur     pretti    prevent \n        NA         NA         NA         NA         NA         NA         NA \n  previous      price    primari      prime   prioriti     prison     privat \n        NA         NA         NA         NA         NA         NA         NA \n  privileg        pro    probabl    problem    process   proclaim     produc \n        NA         NA         NA         NA         NA         NA         NA \n   product     profit    program   progress    project     promis     proper \n        NA         NA         NA         NA         NA         NA         NA \n    propos    protect    protest      proud     provid     public     puerto \n        NA         NA         NA         NA         NA         NA         NA \n      pull \n        NA \n\n\n\n# Extract predictions on training data & observed values\ncomparison_df &lt;- data.frame(train_predictions=mymodel$fitted.values,\nobserved=mymodel$y)\n# Apply prediction threshold\ncomparison_df$train_predictions&lt;-ifelse(comparison_df$train_predictions&gt;=0.5,\nyes = 1,\nno = 0)\n# Compute accuracy (scale: 0-1, 0=0%, 1=100%)\nnrow(comparison_df[comparison_df$train_predictions==comparison_df$observed,]) /\nnrow(comparison_df)\n\n[1] 1\n\n\n\n\n\n\n# format the outcome variable as a factor variable\ntrumpbernie$trump_tweet &lt;- factor(trumpbernie$trump_tweet, levels = c(0,1))\n\n# Use package \"caret\" to do k-fold cross-validation\ntc &lt;- caret::trainControl(method = 'cv', number = 3)\n\nset.seed(12345) # Set to ensure that folds are the same across models\nlinear_model &lt;- caret::train(trump_tweet ~ .,\n                             data = trumpbernie,\n                             method = \"glm\",\n                             trControl = tc,\n                             family = \"binomial\")\n\nWarning: glm.fit: algorithm did not converge\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : prediction from rank-deficient fit; attr(*, “non-estim”) has doubtful cases\n\nlinear_model$results$Accuracy\n\n[1] 0.533384\n\npred_class &lt;- predict(linear_model, newdata = trumpbernie)\ntrain_acc &lt;- mean(pred_class == trumpbernie$trump_tweet)\ntrain_acc\n\n[1] 1\n\n\nAccuracy (I believe this is 1 - the test error) is 0.533384, which means this model is totally guessing\ntraining error is 0\nSo the results are overfitted.\n\n\n\n\nX &lt;- trumpbernie[,-c('trump_tweet'),with=F] # Exclude response and id columns.\nX &lt;- as.matrix(X) # Make input data into a matrix\n\ncvglmnet &lt;- cv.glmnet(x = X, \n                            y = trumpbernie$trump_tweet,\n                            nfolds = 5,\n                            standardize = TRUE,\n                            family='binomial',\n                            alpha=0,\n                            type.measure = 'class')\n\ncvglmnet\n\n\nCall:  cv.glmnet(x = X, y = trumpbernie$trump_tweet, type.measure = \"class\",      nfolds = 5, standardize = TRUE, family = \"binomial\", alpha = 0) \n\nMeasure: Misclassification Error \n\n    Lambda Index Measure       SE Nonzero\nmin  3.276    83 0.08674 0.004135    1488\n1se  4.979    74 0.09073 0.005861    1488\n\ncvglmnet$cvm[cvglmnet$index[[1]]]\n\n[1] 0.08673978\n\ncvglmnet$cvm[cvglmnet$index[[2]]]\n\n[1] 0.09072782\n\n\nMeasure (in the cvglmnet) is the Error, so the accuracy would be 1 - Error.\nFor min, there is a improved accuracy (0.9132602 &gt; 0.533384), this is a better model.\nWith \\(\\lambda\\) increasing, variance decreased and the bias increased, which is worthful!\n\n\n\n\nplot(cvglmnet, sign.lambda = 1)\n\n\n\n\n\n\n\n\n1488 indicates the number of nonzero coefficients is 1488.\n\n\n\n\nbest_coefs &lt;- coef(cvglmnet, s = \"lambda.min\")\nbest_coefs_dt &lt;- data.table(word=rownames(best_coefs),\n                            coef=best_coefs[,1])\nbest_coefs_dt[order(coef,decreasing = T)]\n\n             word       coef\n           &lt;char&gt;      &lt;num&gt;\n   1:     atlanta  0.1492730\n   2:        Npme  0.1425327\n   3:     patriot  0.1371343\n   4:    colorado  0.1347370\n   5:    sacrific  0.1294068\n  ---                       \n1492:      vulner -0.1316845\n1493:        view -0.1323054\n1494:       visit -0.1328930\n1495:      volunt -0.1334961\n1496: (Intercept) -0.1395247\n\n\nAfter extracting the coefficients at lambda.min, I found that words with positive coefficients (such as atlanta and patriot) are more likely to indicate a Trump tweet, while words with negative coefficients (such as volunt and vulner) are more likely to indicate a Bernie tweet. This is generally consistent with my expectations regarding the language styles of the two politicians: Trump’s tweets tend to focus on geographic locations or patriotic topics, whereas Bernie’s tweets emphasize social vulnerability and volunteer-related actions.\n\n\n\n\n\n\n\nsna_data &lt;- fread(\"~/Documents/ML/Labs/Lab1/Kaggle_Social_Network_Ads.csv\")\nsna_data$Purchased &lt;- factor(sna_data$Purchased, levels = c(0,1))\n\n\n\n\n\n# Use package \"caret\" to do k-fold cross-validation\ntc &lt;- caret::trainControl(method = 'cv', number = 5)\n\nset.seed(12345) # Set to ensure that folds are the same across models\nnonlinear_model &lt;- caret::train(Purchased ~ .,\n                             data = sna_data,\n                             method = \"glm\",\n                             trControl = tc,\n                             family = \"binomial\")\n\n\n\nnonlinear_model$results\n\n  parameter  Accuracy     Kappa AccuracySD   KappaSD\n1      none 0.8452399 0.6500518 0.05814504 0.1348846\n\n\ntest error rate is 0.8452399.\n\n\n\n\nset.seed(12345) # Set to ensure that folds are the same across models\ngam_model1 &lt;- caret::train(Purchased ~ ns(Age,2) + ns(Salary, 2),\n                           data = sna_data,\n                           method = \"glm\",\n                           trControl = tc,\n                           family = \"binomial\")\ngam_model1$results\n\n  parameter  Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.9074945 0.7972392 0.02593077 0.05894798\n\ngam_model2 &lt;- caret::train(Purchased ~ ns(Age,3) + ns(Salary, 3),\n                           data = sna_data,\n                           method = \"glm\",\n                           trControl = tc,\n                           family = \"binomial\")\ngam_model2$results\n\n  parameter  Accuracy    Kappa AccuracySD   KappaSD\n1      none 0.8974922 0.775284 0.02239778 0.0475373\n\ngam_model3 &lt;- caret::train(Purchased ~ ns(Age,4) + ns(Salary, 4),\n                           data = sna_data,\n                           method = \"glm\",\n                           trControl = tc,\n                           family = \"binomial\")\ngam_model3$results\n\n  parameter  Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.8976426 0.7764215 0.02760603 0.06264316\n\n\n\n\nAccuracy (test error) is improved! So, yes!\n\n\n\nThis indicates that the previous standard logistic regression model was underfitting. In other words, the earlier linear model was not sufficient to capture the patterns in this dataset, so we need a more flexible model to fit it—namely, the GAM. Compared to the linear model, the GAM introduces more bias.\n\n\n\nI would choose GAM 1, because it achieves the highest test accuracy.\n\n\n\n\n\nfinal_model &lt;- glm(Purchased ~ ns(Age,2) + ns(Salary, 2), data=sna_data, family = \"binomial\")\n\n{\nggpreds &lt;- ggpredict(final_model)\nplot(ggpreds)\n}\n\n$Age\n\n\n\n\n\n\n\n\n\n\n$Salary\n\n\n\n\n\n\n\n\n\nThere is no linear relationship!\n\n\n\nNo, because Ridge or lasso is supposed to increase the bias to deal with overfitting. But this situation is about underfitting, which needs decreasing the bias.\n\n\n\n\nomitted\n\n# dt &lt;- readRDS(\"dt.rds\")\n\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- 2 * x + rnorm(100, mean = 0, sd = 0.5)\ndt &lt;- data.frame(X = x, Y = y)\n\nhead(dt)\n\n           X          Y\n1 -1.6321676 -2.9620389\n2  0.6181952  1.6070036\n3 -0.7632214 -1.1227492\n4 -2.4289158 -3.8943627\n5  1.7724663  3.6673232\n6 -0.5902313 -0.6320544\n\n\n\nmy_lm &lt;- lm(y ~ poly(x,2), data = dt)\n\nchunk2 &lt;- function(x,n) split(x, cut(seq_along(x), n, labels = FALSE))\n\ncv_poly &lt;- function(k, d, dt) {\n  n &lt;- nrow(dt)\n  folds &lt;- chunk2(1:n, k)\n  mse_list &lt;- c()\n  \n  for (i in 1:k){\n    test_idx &lt;- folds[[i]]\n    train_idx &lt;- setdiff(1:n, test_idx)\n    train_data &lt;- dt[train_idx, ]\n    test_data  &lt;- dt[test_idx, ]\n    \n     model &lt;- lm(Y ~ poly(X, d), data = train_data)\n     preds &lt;- predict(model, newdata = test_data)\n     \n     mse &lt;- mean((test_data$Y - preds)^2)\n     mse_list &lt;- c(mse_list, mse)\n  }\n  return(mean(mse_list))\n}\n\ncv_poly(5,1,dt)\n\n[1] 0.2495171"
  },
  {
    "objectID": "notes/ML_lab.html#lecture-0",
    "href": "notes/ML_lab.html#lecture-0",
    "title": "Machine Learning Lab",
    "section": "",
    "text": "library(ISLR2)\nattach(Wage)\n\n\nfit &lt;- lm(wage ~ poly(age, 4), data = Wage)\ncoef(summary(fit))\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    111.70361  0.7287409 153.283015 0.000000e+00\npoly(age, 4)1  447.06785 39.9147851  11.200558 1.484604e-28\npoly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32\npoly(age, 4)3  125.52169 39.9147851   3.144742 1.678622e-03\npoly(age, 4)4  -77.91118 39.9147851  -1.951938 5.103865e-02\n\n\n\nfit2 &lt;- lm(wage ~ poly(age, 4, raw = T), data = Wage)\ncoef(summary(fit2))\n\n                            Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)            -1.841542e+02 6.004038e+01 -3.067172 0.0021802539\npoly(age, 4, raw = T)1  2.124552e+01 5.886748e+00  3.609042 0.0003123618\npoly(age, 4, raw = T)2 -5.638593e-01 2.061083e-01 -2.735743 0.0062606446\npoly(age, 4, raw = T)3  6.810688e-03 3.065931e-03  2.221409 0.0263977518\npoly(age, 4, raw = T)4 -3.203830e-05 1.641359e-05 -1.951938 0.0510386498"
  },
  {
    "objectID": "notes/ML_lab.html#chapter-1",
    "href": "notes/ML_lab.html#chapter-1",
    "title": "Machine Learning",
    "section": "Chapter 1",
    "text": "Chapter 1\nStatistical learning: a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised.\nRegression: predicting a continuous or quantitative output value.\nClassification: predicting a categorical or qualitative output.\nClustering: grouping individuals according to their observed characteristics.\n\nNotation\n\nn — the number of distinct data points or observations\np — the number of variables that are available for use in making predictions\n\\(x_{ij}\\) — the value of the \\(j\\)th variable for the \\(i\\)th observation, where \\(i = 1, 2, \\dots, n\\) and \\(j = 1, 2, \\dots, p\\).\n\\(i\\) — observation index\n\\(j\\) — variable index\n\\(\\mathbf{X}\\) — an \\(n \\times p\\) matrix whose \\((i,j)\\)th element is \\(x_{ij}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs an example, consider\n\\[\nA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}.\n\\] Then\n\\[\nAB =\n\\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}\n\\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\times 5 + 2 \\times 7 & 1 \\times 6 + 2 \\times 8 \\\\\n3 \\times 5 + 4 \\times 7 & 3 \\times 6 + 4 \\times 8\n\\end{pmatrix}\n=\n\\begin{pmatrix} 19 & 22 \\\\ 43 & 50 \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "notes/ML_lab.html#chapter-2",
    "href": "notes/ML_lab.html#chapter-2",
    "title": "Machine Learning",
    "section": "Chapter 2",
    "text": "Chapter 2\n\\(X\\) — input variables, predictors, independent variables, features, or just variables\n\\(Y\\) — response, dependent variable\n\\(Y = f(X) + \\epsilon\\), where \\(f\\) is some fixed but unknown function of \\(X\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero (f represents the systematic information that \\(X\\) provides about \\(Y\\).).\nstatistical learning — a set of approaches for estimating \\(f\\)\n\n2.1.1 Why Estimate \\(f\\)?\n\nPrediction\n\\[\\hat{Y} = \\hat{f}(X)\\]\n\\(\\hat{f}\\) — our estimate for \\(f\\)\n\\(\\hat{Y}\\) — the resulting prediction for \\(Y\\)\nThe accuracy of \\(\\hat{Y}\\) as a prediction of \\(Y\\) depends on two quantities, reducible error and irreducible error.\nreducible error — We can potentially improve the accuracy of \\(\\hat{f}\\) by using the most appropriate statistical learning technique to estimate \\(f\\).\nirreducible error — introduced by \\(\\epsilon\\) (\\(\\hat{Y} = f(X)\\), \\(Y = f(X) + \\epsilon\\)); this error is lager than zero, because of unmeasured variables and unmeasurable variation in \\(\\epsilon\\).\nWe have\n\\[\n\\begin{aligned}\n\\mathrm{Var}(X)\n&= E\\big[(X - E[X])^2\\big] \\\\\n%&= E\\big[X^2 - 2X E[X] + (E[X])^2\\big] \\\\\n%&= E[X^2] - 2E[X] E[X] + (E[X])^2 \\\\\n&= E[X^2] - (E[X])^2\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\n\\mathrm{Var}(\\epsilon)\n&= E[\\epsilon^2] - 0^2 \\\\\n&= E[\\epsilon^2]\n\\end{aligned}\n\\]\nThen\n\\[\n\\begin{aligned}\nE(Y - \\hat{Y})^2\n&= E\\big[ f(X) + \\epsilon - \\hat{f}(X) \\big]^2 \\\\\n&= E\\big[ (f(X) - \\hat{f}(X)) + \\epsilon \\big]^2 \\\\\n&= E\\big[ (f(X) - \\hat{f}(X))^2 + 2(f(X) - \\hat{f}(X))\\epsilon + \\epsilon^2 \\big] \\\\\n&= (f(X) - \\hat{f}(X))^2 + E(\\epsilon^2)\\\\\n&= (f(X) - \\hat{f}(X))^2 + \\mathrm{Var}(\\epsilon)\n\\end{aligned}\n\\]\nThat is\n\\[\nE(Y - \\hat{Y})^2\n= \\underbrace{(f(X) - \\hat{f}(X))^2}_{\\mathrm{Reducible}}\n    + \\underbrace{\\mathrm{Var}(\\epsilon)}_{\\mathrm{Irreducible}}\n\\]\n\n\nInference\nWe are often interested in understanding the association between \\(Y\\) and \\(X\\).\nNow \\(\\hat{f}\\) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:\n\nWhich predictors are associated with the response?\nWhat is the relationship between the response and each predictor?\nCan the relationship between \\(Y\\) and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n\nFor example,\n\nWhich media are associated with sales?\nWhich media generate the biggest boost in sales?\nHow large of an increase in sales is associated with a given increase in TV advertising?\n\nAn example of modeling for inference — to what extent is the product’s price associated with sales?\nIn a real estate setting,\n\nHow much extra will a house be worth if it has a view of the river? (inference)\nIs this house under- or over-valued? (prediction)\n\nIn other words (ChatGPT),\n\nbetter understanding the relationship between the response and the predictors -&gt; Inference;\naccurately predicting the response for future observations -&gt; Prediction.\n\n\n\n\n2.1.2 How do we estimate \\(f\\)?\n\nParametric Methods\n\nWe make an assumption about the functional form, or shape, of \\(f\\). A very simple assumption is that \\(f\\) is linear in \\(X\\) (linear model):\n\n\\[\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\n\\]\n\nAfter a model has been selected, we need a procedure that uses the training data to fit or train the model. We need to estimate the parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\)\n\nThat is\n\\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\n\\]\nThe most common approach to fitting the model is referred to as ordinary least squares (OLS).\n\n\nNon-Parametric Methods\nNon-parametric methods do not make explicit assumptions about the functional form of \\(f\\). Instead they seek an estimate of \\(f\\).\nSince they do not reduce the problem of estimating \\(f\\) to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for \\(f\\).\n\nthin-plate spline\n\nMore details in Chapter 7\n\n\n\n2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability\nOne question: Why would we ever choose to use a more restrictive method instead of a very flexible approach?\nAnswer: We might choose a more restrictive method because it is easier to interpret. When our goal is inference, understanding the relationship between each predictor and the response is important, and very flexible methods can produce estimates that are too complex to interpret.\nSurprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. (due to overfitting in highly flexbile methods)\n\n\n2.1.5 Regression Versus Classification Problems\nChoice of statistical learning method mainly depends on the type of response:\n\nQuantitative response → linear regression\nQualitative response → logistic regression\n\nPredictor type (quantitative or qualitative) is generally less important, as long as any qualitative predictors are properly coded before analysis.\n\n\n2.2.1 Measuring the Quality of Fit\nWe need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE):\n\\[\n\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_i - \\hat{f}(x_i)\\big)^2\n\\] But this MSE above is training MSE, which we don’t care about.\nWe are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.\nThus, we could compute the average squared prediction error for these test observations \\((x_0, y_0)\\):\n\\[\n\\mathrm{Ave}\\big( (y_0 - \\hat{f}(x_0))^2 \\big)\n\\]\nwhere \\((x_0, y_0)\\) is a previously unseen test observation not used to train the statistical learning method.\nAs model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.\nOne important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.\n\n\n2.2.2 The Bias-Variance Trade-Off\nFor a given value \\(x_0\\), the expected test MSE can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\), and the variance of the error terms \\(\\epsilon\\)\nThat is (bias–variance trade-off)\n\\[\nE\\big( y_0 - \\hat{f}(x_0) \\big)^2\n= \\mathrm{Var}\\big(\\hat{f}(x_0)\\big)\n+ \\big[ \\mathrm{Bias}\\big(\\hat{f}(x_0)\\big) \\big]^2\n+ \\mathrm{Var}(\\epsilon)\n\\]\nThis equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias.\nvariance — the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set\nIn general, more flexible statistical methods have higher variance.\nbias — the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model\nIn general, more flexible methods result in less bias.\n\n\n2.2.3 The Classification Setting\ntraining error rate (the fraction of incorrect classifications)\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)\n\\] Here, \\(\\hat{y}_i\\) is the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\). And \\(I(y_i \\neq \\hat{y}_i)\\) is an indicator variable that equals 1 if \\(y_i \\neq \\hat{y}_i\\) and 0 if \\(y_i = \\hat{y}_i\\). If \\(I(y_i \\neq \\hat{y}_i) = 0\\), then the \\(i\\)th observation was classified correctly by our classification method; otherwise it was misclassified.\ntest error rate\n\\[\n\\mathrm{Ave}\\big(I(y_0 \\neq \\hat{y}_0)\\big)\n\\]"
  },
  {
    "objectID": "notes/ML_lab.html#chapter-5",
    "href": "notes/ML_lab.html#chapter-5",
    "title": "Machine Learning",
    "section": "Chapter 5",
    "text": "Chapter 5\nResampling methods: cross-validation, bootstrap\ncross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.\nThe bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.\nmodel assessment — the process of evaluating a model’s performance\nmodel selection — the process of selecting the proper level of flexibility for a model\n\nThe Validation Set Approach\nSplitting the set of observations into the training set and the validation set.\nTwo drawback\n\nthe validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nthe validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set (only a subset of the observations, the training set, are used to fit the model—since statistical methods tend to perform worse when trained on fewer observations).\n\ncross-validation — a refinement of the validation set approach\n\n\nLeave-One-Out Cross-Validation\n\nvalidation set — \\((x_1, y_1)\\)\ntraining set — \\(\\{(x_2, y_2), \\dots, (x_n, y_n)\\}\\)\nThe statistical learning method is fit on the n − 1 training observations, repeat n times with a different validation observation.\n\nIn other words,\nIn the \\(i\\)-th iteration of LOOCV, the training set contains all observations except the \\(i\\)-th one:\n\\[\n\\mathrm{Training set}_i = (x_1, y_1), \\dots, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \\dots, (x_n, y_n)\n\\]\nThe validation set consists of the single observation \\((x_i, y_i)\\). This procedure is repeated for \\(i = 1, \\dots, n\\).\nRepeating this approach \\(n\\) times produces \\(n\\) squared errors, \\(\\mathrm{MSE}_1, \\dots, \\mathrm{MSE}_n\\). The LOOCV estimate for the test \\(\\mathrm{MSE}\\) is the average of these \\(n\\) test error estimates:\n\\[\n\\mathrm{CV}(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{MSE}_i\n\\] With least squares linear or polynomial regression:\n\\[\n\\mathrm{CV}(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\n\\] where \\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the original least squares fit, and \\(h_i\\) is the leverage defined in (3.37) on page 99.\n\n\nk-Fold Cross-Validation\nk-fold CV involves randomly dividing the set of observations into k groups, or folds, of approximately equal size.\nThe \\(k\\)-fold CV estimate is computed by averaging these values:\n\\[\n\\mathrm{CV}(k) = \\frac{1}{k} \\sum_{i=1}^{k} \\mathrm{MSE}_i.\n\\]\nLOOCV is a special case of k-fold CV in which k is set to equal n.\nIn practice, one typically performs k-fold CV using k = 5 or k = 10.\nThe goal here is to identify the method that results in the lowest test error; for this purpose, the location of the minimum point on the estimated test \\(\\mathrm{MSE}\\) curve is important, while the actual value of the estimated \\(\\mathrm{MSE}\\) is not important.\n\n\nBias-Variance Trade-Off for k-Fold Cross-Validation\nLOOCV — low bias (approximately unbiased estimates) but high variance!\nSince the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.\n\n\nCross-Validation on Classification Problems\nIn the classification setting, the LOOCV error rate takes the form\n\\[\n\\mathrm{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{Err}_i\n\\] where \\(\\mathrm{Err}_i = I(y_i \\neq \\hat{y}_i)\\)\nIn practice, for real data, the Bayes decision boundary and the test error rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? We can use cross-validation in order to make this decision."
  },
  {
    "objectID": "notes/ML_lab.html#chapter-6",
    "href": "notes/ML_lab.html#chapter-6",
    "title": "Machine Learning",
    "section": "Chapter 6",
    "text": "Chapter 6\nHow to improve the linear model given that it has distinct advantages in terms of inference and, on real-world problems, is often surprisingly competitive in relation to non-linear methods.\nWhy? Why might we want to use another fitting procedure instead of least squares?\nBecause — alternative fitting procedures can yield better prediction accuracy and model interpretability.\n\nPrediction Accuracy\nModel Interpretability\n\nthree important classes of methods, alternatives to using least squares to fit:\n\nSubset Selection\nShrinkage\nDimension Reduction\n\n\nSubset Selection\nusing a subset of the original variables\nomitted\n\n\nShrinkage Methods\nshrinking variables’ coefficients toward zero\n\nRidge Regression\nleast squares — estimates coefficients that minimize\n\\[\n\\mathrm{RSS} =  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2\n\\]\nRidge regression — estimates coefficients that minimize\n\\[\n\\mathrm{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] where \\(\\lambda \\geq 0\\) is a tuning parameter. The second term, called a shrinkage penalty, is small when \\(\\beta_1, \\dots, \\beta_p\\) are close to zero.\nridge regression will produce a different set of coefficient estimates, \\(\\hat{\\beta}^{R}_{\\lambda}\\), for each value of \\(\\lambda\\).\nNoticing that the shrinkage penalty is applied not to the intercept \\(\\beta_0\\).\nTaking the derivative of the RSS gives\n\\[\\beta_0 = \\bar{y} - \\sum_{j=1}^p \\beta_j \\bar{x}_j\\]\nassuming that the variables have been centered to have mean zero, that is, \\(\\bar{x}_j = 0\\).\nThen\n\\[\n\\beta_0 = \\bar{y} = \\sum_{i=1}^n y_i / n\n\\]\nWhy Does Ridge Regression Improve Over Least Squares?\nRidge regression’s advantage over least squares is rooted in the bias-variance trade-off. As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n\n\nThe Lasso\nRidge regression’s drawback — Increasing the value of \\(\\lambda\\) will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.\nThe lasso coefficients, \\(\\hat{\\beta}_\\lambda^L\\), minimize the quantity\n\\[\n\\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\nAnother Formulation for Ridge Regression and the Lasso\nRidge Regression\n\\[\n\\min_{\\beta}\n\\left\\{\n\\sum_{i=1}^{n} \\Big(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\Big)^2\n\\right\\}\n\\quad \\text{subject to }\n\\sum_{j=1}^{p} \\beta_j^2 \\le s\n\\] Lasso\n\\[\n\\min_{\\beta}\n\\left\\{\n\\sum_{i=1}^{n} \\Big(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\Big)^2\n\\right\\}\n\\quad \\text{subject to }\n\\sum_{j=1}^{p} |\\beta_j| \\le s\n\\]\nbest subset selection\n\\[\n\\min_{\\beta}\n\\left\\{\n\\sum_{i=1}^{n} \\Big(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\Big)^2\n\\right\\}\n\\quad \\text{subject to }\n\\sum_{j=1}^{p} I(\\beta_j \\neq 0) \\le s\n\\]\nThis is an exponentially large combinatorial search problem, which is computationally infeasible. In contrast, Ridge regression and Lasso are convex optimization problems.\nThe Variable Selection Property of the Lasso\nconstraint regions\n\\[\n|\\beta_1| + |\\beta_2| \\le s \\quad \\text{and} \\quad \\beta_1^2 + \\beta_2^2 \\le s\n\\]\nSee Figure 6.7 for details.\nComparing the Lasso and Ridge Regression\nIn general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.\nmy question — why fig 6.9. plots against \\(R^2\\)?\nA Simple Special Case for Ridge Regression and the Lasso\n\nridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.\n\n\n\nSelecting the Tuning Parameter\nBy cross-validation\n\n\n\nConsiderations in High Dimensions\nhigh-dimensional setting — the case where the number of features \\(p\\) is larger than the number of observations \\(n\\)\n\nWhat Goes Wrong in High Dimensions?\nThe problem is simple: when \\(p &gt; n\\) or \\(p \\approx n\\), a simple least squares regression line is too flexible and hence overfits the data.\nIn high-dimensional settings (when \\(p \\ge n\\)), traditional methods for adjusting training set error or \\(R^2\\)—like \\(C_p\\), \\(\\mathrm{AIC}\\), \\(\\mathrm{BIC}\\), or adjusted \\(R^2\\)—fail because estimating \\(\\hat{\\sigma}^2\\) becomes unreliable, and adjusted \\(R^2\\) can misleadingly reach 1. Therefore, specialized methods are needed for analyzing high-dimensional data.\n\n\nRegression in High Dimensions\nEssentially, these approaches (ridge regression, lasso) avoid overfitting by using a less flexible fitting approach than least squares.\n\nregularization or shrinkage plays a key role in high-dimensional problems,\nappropriate tuning parameter selection is crucial for good predictive performance,\nthe test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.\n\ncurse of dimensionality — they can lead to improved predictive models if these features are in fact relevant to the problem at hand, but will lead to worse results if the features are not relevant. Even if they are relevant, the variance incurred in fitting their coefficients may outweigh the reduction in bias that they bring.\n\n\nInterpreting Results in High Dimensions\nmulticollinearity — the variables in a regression might be correlated with each other\nIn the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model.\nTo make it clear that what we have identified is simply one of many possible models for predicting blood pressure, and that it must be further validated on independent data sets.\nIt is also important to be particularly careful in reporting errors and measures of model fit in the high-dimensional setting. We have seen that when \\(p &gt; n\\), it is easy to obtain a useless model that has zero residuals. Therefore, one should never use sum of squared errors, p-values, \\(R^2\\) statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting.\nIt is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R² on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not."
  },
  {
    "objectID": "notes/ML_lab.html#chapter-7",
    "href": "notes/ML_lab.html#chapter-7",
    "title": "Machine Learning",
    "section": "Chapter 7",
    "text": "Chapter 7\n\nPolynomial Regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\cdots + \\beta_d x_i^d + \\varepsilon_i\n\\]\n\n\nStep Functions\nWe have\n\\[\n\\begin{aligned}\nC_0(X) &= I(X &lt; c_1), \\\\\nC_1(X) &= I(c_1 \\le X &lt; c_2), \\\\\nC_2(X) &= I(c_2 \\le X &lt; c_3), \\\\\n&\\;\\;\\vdots \\\\\nC_{K-1}(X) &= I(c_{K-1} \\le X &lt; c_K), \\\\\nC_K(X) &= I(c_K \\le X),\n\\end{aligned}\n\\]\nThen\n\\[\ny_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i) + \\cdots + \\beta_K C_K(x_i) + \\varepsilon_i.\n\\]\n\n\nBasis Functions\nPolynomial and piecewise-constant regression models are in fact special cases of a basis function approach.\nThe idea is to have at hand a family of functions or transformations that can be applied to a variable \\(X\\): \\(b_1(X), b_2(X), \\dots, b_K(X)\\).\nFit the model\n\\[\ny_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\beta_3 b_3(x_i) + \\cdots + \\beta_K b_K(x_i) + \\varepsilon_i\n\\]\npolynomial regression — \\(b_j(x_i) = x_i^j\\)\nstep functions / piecewise constant functions — \\(b_j(x_i) = I(c_j \\le x_i &lt; c_{j+1})\\)\n\n\nRegression Splines\npiecewise polynomial + smooth constraint\n\n\nGeneralized Additive Models\n\nGAMs for Regression Problems\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\sum_{j=1}^{p} f_j(x_{ij}) + \\epsilon_i \\\\\n    &= \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\cdots + f_p(x_{ip}) + \\epsilon_i\n\\end{aligned}\n\\]\n\n\nGAMs for Classification Problems\n\\[\n\\log \\frac{p(X)}{1 - p(X)} = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p)\n\\]\n\n\nPros and Cons of GAMs\nAdvantages:\n\nGAMs allow fitting a non-linear function \\(f_j\\) to each predictor \\(X_j\\), automatically capturing non-linear relationships that standard linear regression might miss. This removes the need to manually try multiple transformations for each variable.\nThe non-linear fits can lead to more accurate predictions for the response \\(Y\\).\nThe additive nature of the model allows examination of the effect of each \\(X_j\\) on \\(Y\\) individually, while holding other variables fixed.\nThe smoothness of each function \\(f_j\\) can be summarized using degrees of freedom.\n\nLimitations:\n\nGAMs are restricted to additive effects. With many predictors, important interactions may be missed.\nInteractions can be manually added, either as:\n\nInteraction terms \\(X_j \\times X_k\\), or\n\nLow-dimensional interaction functions \\(f_{jk}(X_j, X_k)\\), which can be fit using two-dimensional smoothers such as local regression or two-dimensional splines (not covered here)."
  },
  {
    "objectID": "notes/ML_lab.html#part-1-bernie-sanders-and-donald-trump-tweets.",
    "href": "notes/ML_lab.html#part-1-bernie-sanders-and-donald-trump-tweets.",
    "title": "Machine Learning Lab",
    "section": "",
    "text": "library(data.table)\nlibrary(glmnet)\nlibrary(ggplot2)\nlibrary(mlbench)\nlibrary(caret)\nlibrary(splines)\nlibrary(ggeffects)\n\n\n# load data\ntrumpbernie &lt;- fread(file = \"~/Documents/ML/Labs/Lab1/trumpbernie.csv\")\n\n# inspect\n# trumpbernie[1:10,1:10]\ndim(trumpbernie) \n\n[1] 1003 1496\n\n\nThe dataset is 1003-row and 1496-column one. The number of variables (1496) &gt; the number of observation (1003), so this dataset is high-dimensional. Based on this, a standard logistic regression won’t work well for the purpose of prediction due to overfitting and multilinearity.\n\n\n\n\nmymodel &lt;- glm(trump_tweet ~ ., data = trumpbernie, family=\"binomial\")\n\nWarning: glm.fit: algorithm did not converge\n\nsum(!is.na(coef(mymodel)))\n\n[1] 1001\n\ncoef(mymodel)[(1001:1050)]\n\n     popul    popular    possibl       post    potenti    poverti      power \n 349.33967 -771.91767  -30.26994   25.75710 -380.91299   59.38732         NA \n   practic     prayer     prefer    premium     prepar       pres  prescript \n        NA         NA         NA         NA         NA         NA         NA \n   present     presid presidenti      press    pressur     pretti    prevent \n        NA         NA         NA         NA         NA         NA         NA \n  previous      price    primari      prime   prioriti     prison     privat \n        NA         NA         NA         NA         NA         NA         NA \n  privileg        pro    probabl    problem    process   proclaim     produc \n        NA         NA         NA         NA         NA         NA         NA \n   product     profit    program   progress    project     promis     proper \n        NA         NA         NA         NA         NA         NA         NA \n    propos    protect    protest      proud     provid     public     puerto \n        NA         NA         NA         NA         NA         NA         NA \n      pull \n        NA \n\n\n\n# Extract predictions on training data & observed values\ncomparison_df &lt;- data.frame(train_predictions=mymodel$fitted.values,\nobserved=mymodel$y)\n# Apply prediction threshold\ncomparison_df$train_predictions&lt;-ifelse(comparison_df$train_predictions&gt;=0.5,\nyes = 1,\nno = 0)\n# Compute accuracy (scale: 0-1, 0=0%, 1=100%)\nnrow(comparison_df[comparison_df$train_predictions==comparison_df$observed,]) /\nnrow(comparison_df)\n\n[1] 1\n\n\n\n\n\n\n# format the outcome variable as a factor variable\ntrumpbernie$trump_tweet &lt;- factor(trumpbernie$trump_tweet, levels = c(0,1))\n\n# Use package \"caret\" to do k-fold cross-validation\ntc &lt;- caret::trainControl(method = 'cv', number = 3)\n\nset.seed(12345) # Set to ensure that folds are the same across models\nlinear_model &lt;- caret::train(trump_tweet ~ .,\n                             data = trumpbernie,\n                             method = \"glm\",\n                             trControl = tc,\n                             family = \"binomial\")\n\nWarning: glm.fit: algorithm did not converge\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : prediction from rank-deficient fit; attr(*, “non-estim”) has doubtful cases\n\nlinear_model$results$Accuracy\n\n[1] 0.533384\n\npred_class &lt;- predict(linear_model, newdata = trumpbernie)\ntrain_acc &lt;- mean(pred_class == trumpbernie$trump_tweet)\ntrain_acc\n\n[1] 1\n\n\nAccuracy (I believe this is 1 - the test error) is 0.533384, which means this model is totally guessing\ntraining error is 0\nSo the results are overfitted.\n\n\n\n\nX &lt;- trumpbernie[,-c('trump_tweet'),with=F] # Exclude response and id columns.\nX &lt;- as.matrix(X) # Make input data into a matrix\n\ncvglmnet &lt;- cv.glmnet(x = X, \n                            y = trumpbernie$trump_tweet,\n                            nfolds = 5,\n                            standardize = TRUE,\n                            family='binomial',\n                            alpha=0,\n                            type.measure = 'class')\n\ncvglmnet\n\n\nCall:  cv.glmnet(x = X, y = trumpbernie$trump_tweet, type.measure = \"class\",      nfolds = 5, standardize = TRUE, family = \"binomial\", alpha = 0) \n\nMeasure: Misclassification Error \n\n    Lambda Index Measure       SE Nonzero\nmin  3.276    83 0.08674 0.004135    1488\n1se  4.979    74 0.09073 0.005861    1488\n\ncvglmnet$cvm[cvglmnet$index[[1]]]\n\n[1] 0.08673978\n\ncvglmnet$cvm[cvglmnet$index[[2]]]\n\n[1] 0.09072782\n\n\nMeasure (in the cvglmnet) is the Error, so the accuracy would be 1 - Error.\nFor min, there is a improved accuracy (0.9132602 &gt; 0.533384), this is a better model.\nWith \\(\\lambda\\) increasing, variance decreased and the bias increased, which is worthful!\n\n\n\n\nplot(cvglmnet, sign.lambda = 1)\n\n\n\n\n\n\n\n\n1488 indicates the number of nonzero coefficients is 1488.\n\n\n\n\nbest_coefs &lt;- coef(cvglmnet, s = \"lambda.min\")\nbest_coefs_dt &lt;- data.table(word=rownames(best_coefs),\n                            coef=best_coefs[,1])\nbest_coefs_dt[order(coef,decreasing = T)]\n\n             word       coef\n           &lt;char&gt;      &lt;num&gt;\n   1:     atlanta  0.1492730\n   2:        Npme  0.1425327\n   3:     patriot  0.1371343\n   4:    colorado  0.1347370\n   5:    sacrific  0.1294068\n  ---                       \n1492:      vulner -0.1316845\n1493:        view -0.1323054\n1494:       visit -0.1328930\n1495:      volunt -0.1334961\n1496: (Intercept) -0.1395247\n\n\nAfter extracting the coefficients at lambda.min, I found that words with positive coefficients (such as atlanta and patriot) are more likely to indicate a Trump tweet, while words with negative coefficients (such as volunt and vulner) are more likely to indicate a Bernie tweet. This is generally consistent with my expectations regarding the language styles of the two politicians: Trump’s tweets tend to focus on geographic locations or patriotic topics, whereas Bernie’s tweets emphasize social vulnerability and volunteer-related actions."
  },
  {
    "objectID": "notes/ML_lab.html#part-2-social-network-ad-purchase",
    "href": "notes/ML_lab.html#part-2-social-network-ad-purchase",
    "title": "Machine Learning Lab",
    "section": "",
    "text": "sna_data &lt;- fread(\"~/Documents/ML/Labs/Lab1/Kaggle_Social_Network_Ads.csv\")\nsna_data$Purchased &lt;- factor(sna_data$Purchased, levels = c(0,1))\n\n\n\n\n\n# Use package \"caret\" to do k-fold cross-validation\ntc &lt;- caret::trainControl(method = 'cv', number = 5)\n\nset.seed(12345) # Set to ensure that folds are the same across models\nnonlinear_model &lt;- caret::train(Purchased ~ .,\n                             data = sna_data,\n                             method = \"glm\",\n                             trControl = tc,\n                             family = \"binomial\")\n\n\n\nnonlinear_model$results\n\n  parameter  Accuracy     Kappa AccuracySD   KappaSD\n1      none 0.8452399 0.6500518 0.05814504 0.1348846\n\n\ntest error rate is 0.8452399.\n\n\n\n\nset.seed(12345) # Set to ensure that folds are the same across models\ngam_model1 &lt;- caret::train(Purchased ~ ns(Age,2) + ns(Salary, 2),\n                           data = sna_data,\n                           method = \"glm\",\n                           trControl = tc,\n                           family = \"binomial\")\ngam_model1$results\n\n  parameter  Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.9074945 0.7972392 0.02593077 0.05894798\n\ngam_model2 &lt;- caret::train(Purchased ~ ns(Age,3) + ns(Salary, 3),\n                           data = sna_data,\n                           method = \"glm\",\n                           trControl = tc,\n                           family = \"binomial\")\ngam_model2$results\n\n  parameter  Accuracy    Kappa AccuracySD   KappaSD\n1      none 0.8974922 0.775284 0.02239778 0.0475373\n\ngam_model3 &lt;- caret::train(Purchased ~ ns(Age,4) + ns(Salary, 4),\n                           data = sna_data,\n                           method = \"glm\",\n                           trControl = tc,\n                           family = \"binomial\")\ngam_model3$results\n\n  parameter  Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.8976426 0.7764215 0.02760603 0.06264316\n\n\n\n\nAccuracy (test error) is improved! So, yes!\n\n\n\nThis indicates that the previous standard logistic regression model was underfitting. In other words, the earlier linear model was not sufficient to capture the patterns in this dataset, so we need a more flexible model to fit it—namely, the GAM. Compared to the linear model, the GAM introduces more bias.\n\n\n\nI would choose GAM 1, because it achieves the highest test accuracy.\n\n\n\n\n\nfinal_model &lt;- glm(Purchased ~ ns(Age,2) + ns(Salary, 2), data=sna_data, family = \"binomial\")\n\n{\nggpreds &lt;- ggpredict(final_model)\nplot(ggpreds)\n}\n\n$Age\n\n\n\n\n\n\n\n\n\n\n$Salary\n\n\n\n\n\n\n\n\n\nThere is no linear relationship!\n\n\n\nNo, because Ridge or lasso is supposed to increase the bias to deal with overfitting. But this situation is about underfitting, which needs decreasing the bias."
  },
  {
    "objectID": "notes/ML_lab.html#part-3-k-fold-cross-validation-from-scratch-bonus",
    "href": "notes/ML_lab.html#part-3-k-fold-cross-validation-from-scratch-bonus",
    "title": "Machine Learning Lab",
    "section": "",
    "text": "omitted\n\n# dt &lt;- readRDS(\"dt.rds\")\n\nx &lt;- rnorm(100, mean = 0, sd = 1)\ny &lt;- 2 * x + rnorm(100, mean = 0, sd = 0.5)\ndt &lt;- data.frame(X = x, Y = y)\n\nhead(dt)\n\n           X          Y\n1 -1.6321676 -2.9620389\n2  0.6181952  1.6070036\n3 -0.7632214 -1.1227492\n4 -2.4289158 -3.8943627\n5  1.7724663  3.6673232\n6 -0.5902313 -0.6320544\n\n\n\nmy_lm &lt;- lm(y ~ poly(x,2), data = dt)\n\nchunk2 &lt;- function(x,n) split(x, cut(seq_along(x), n, labels = FALSE))\n\ncv_poly &lt;- function(k, d, dt) {\n  n &lt;- nrow(dt)\n  folds &lt;- chunk2(1:n, k)\n  mse_list &lt;- c()\n  \n  for (i in 1:k){\n    test_idx &lt;- folds[[i]]\n    train_idx &lt;- setdiff(1:n, test_idx)\n    train_data &lt;- dt[train_idx, ]\n    test_data  &lt;- dt[test_idx, ]\n    \n     model &lt;- lm(Y ~ poly(X, d), data = train_data)\n     preds &lt;- predict(model, newdata = test_data)\n     \n     mse &lt;- mean((test_data$Y - preds)^2)\n     mse_list &lt;- c(mse_list, mse)\n  }\n  return(mean(mse_list))\n}\n\ncv_poly(5,1,dt)\n\n[1] 0.2495171"
  },
  {
    "objectID": "notes/ML.html#chapter-8",
    "href": "notes/ML.html#chapter-8",
    "title": "Machine Learning",
    "section": "Chapter 8",
    "text": "Chapter 8\n\\[\n\\text{Predictor space} \\xrightarrow{\\text{stratifying or segmenting}} \\text{Simple regions}\n\\]\n\n8.1 The Basics of Decision Trees\n\n8.1.1 Regression Trees\nA simple example\nWe use the Hitters data set to predict a baseball player’s Salary based on Years and Hits.\nOverall, the tree stratifies or segments the players into three regions of predictor space:\n\\[\nR_1 = \\{ X \\mid \\text{Years} &lt; 4.5 \\},\nR_2 = \\{ X \\mid \\text{Years} \\ge 4.5, \\ \\text{Hits} &lt; 117.5 \\},\nR_3 = \\{ X \\mid \\text{Years} \\ge 4.5, \\ \\text{Hits} \\ge 117.5 \\}\n\\]\n\n\nPrediction via Stratification of the Feature Space\nHow to build a regression tree\n\nWe divide the predictor space—the set of possible values for \\(X_1, X_2, \\dots, X_p\\)—into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\dots, R_J\\)\nFor every observation that falls into the region \\(R_j\\), we make the same prediction,\nwhich is simply the mean of the response values for the training observations in \\(R_j\\).\n\nThe goal is to find boxes \\(R_1, \\dots, R_J\\) that minimize the RSS, given by\n\\[\n\\mathrm{RSS} = \\sum_{j=1}^J \\sum_{i \\in R_j} \\bigl(y_i - \\hat{y}_{R_j}\\bigr)^2\n\\] where \\(\\hat{y}_{R_j}\\) is the mean response for the training observations within the \\(j\\)th box.\nNotes\n\nthis is computationally infeasible\ntop-down greedy approach which is recursive binary splitting\n\nrecursive binary splitting\n\nselecting the predictor \\(X_j\\) and the cutpoint \\(s\\) such that spliiting the predicator space into regions \\(\\{X|X_j&lt;s\\}\\) and \\(\\{X|X_j \\ge s\\}\\) leads to the greatest possible reduction in RSS. That is, we consider all predictors \\(X_1, \\dots, X_p\\), and all possible values of the output \\(s\\) for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS.\n\n\\[\n\\sum_{i: x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2,\n\\]\n\n\nTree Pruning\na better strategy is to grow a very large tree \\(T_0\\), and then prune it back in order to obtain a subtree.\nCost complexity pruning (weakest link pruning)\n\nRather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\)\n\nFor each value of α there corresponds a subtree \\(T \\subset T_0\\) such that\n\\[\n\\sum_{m=1}^{T} \\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|\n\\]\nas we increase \\(\\alpha\\) from zero in the above expression, branches get pruned from the tree in a nested and predictable fashion\n\n\n8.1.2 Classification Trees\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\nInterpreting the results\n\nthe class prediction corresponding to a particular terminal node region\nthe class propotions among the training observations that fall into that region\n\nKey concept\n\nClassification error rate: A natural alternative to RSS (Residual Sum of Squares) when growing classification trees using recursive binary splitting. However, it is not sufficiently sensitive for tree-growing.\n\n\\[\nE_m = 1 - \\max_{k} (\\hat{p}_{mk})\n\\]\n\nGini index: a measure of node purity. The Gini index takes on a small value if all of the \\(\\hat{p}_{mk}\\)’s are close to zero or one.\n\n\\[\nG = \\sum_{k=1}^{K} \\hat{p}_{mk} (1 - \\hat{p}_{mk})\n\\]\n\nEntropy: the entropy will take on a value near zero if the \\(\\hat{p}_{mk}\\)’s are all near zero or near one.\n\n\\[\nD = - \\sum_{k=1}^{K} \\hat{p}_{mk} \\log\\hat{p}_{mk}\n\\]\nAny of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.\n\n\n8.1.3 Trees Versus Linear Models\n\\[\nf(X) = \\sum_{m=1}^{M} c_m \\cdot 1(X \\in R_m)\n\\]\nconsiderations\n\nthe true decision boundary is linear or non-linear.\ntest error\ninterpretability\nvisualization\n\n\n\n8.1.4 Advantages and Disadvantages of Trees\npros\n\nTrees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\nSome people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.\nTrees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).\nTrees can easily handle qualitative predictors without the need to create dummy variables.\n\ncons\n\nUnfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\nAdditionally, trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree.\n\nHowever, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved.\n\n\n\n8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees\n\n8.2.1 Bagging (Bootstrap aggregation)\nsimple idea: averaging a set of observations reduces variance.\nto take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions.\n\n\nOut-of-Bag Error Estimation\n可以自然地直接用oob error来估计测试误差 而不需要交叉验证或者验证集\n\n\nVariable Importance Measures\nbagging improves prediction accuracy at the expense of interpretability.\none can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees)\n\nIn the case of bagging regression trees，the total amount that the RSS is decreased.\nIn the case of bagging classification trees, the total amount that the Gini index is decreased.\n\n\n\n8.2.2 Random Forests\nforcing each split to consider only a subset of the predictors\nin building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors.\nWe can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.\nThe main difference between bagging (\\(m = p\\)) and random forests (\\(m = \\sqrt{p}\\)) is the choice of predictor subset size \\(m\\).\nThe null rate results from simply classifying each observation to the dominant class overall\n\n\nBoosting\nBoosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\nBoosting learns slowly, unlike fitting a single large tree which may overfit.\nAt each step, a small tree is fitted to the residuals of the current model, not directly to Y.\nThe new tree is added to the model to update the residuals, gradually improving areas where the model performs poorly.\nTree size is small (controlled by parameter \\(d\\)) and learning speed is further slowed by the shrinkage parameter \\(\\lambda\\).\nUnlike bagging, each tree depends on the trees grown before it.\n\n\n8.2.5\nSummary of Tree Ensemble Methods\n\nIn bagging, the trees are grown independently on random samples of the observations. Consequently, the trees tend to be quite similar to each other. Thus, bagging can get caught in local optima and can fail to thoroughly explore the model space.\nIn random forests, the trees are once again grown independently on random samples of the observations. However, each split on each tree is performed using a random subset of the features, thereby decorrelating the trees, and leading to a more thorough exploration of model space relative to bagging.\nIn boosting, we only use the original data, and do not draw any random samples. The trees are grown successively, using a “slow” learning approach: each new tree is fit to the signal that is left over from the earlier trees, and shrunken down before it is used."
  },
  {
    "objectID": "notes/ML.html#chapter-1-1",
    "href": "notes/ML.html#chapter-1-1",
    "title": "Machine Learning",
    "section": "Chapter 1",
    "text": "Chapter 1"
  },
  {
    "objectID": "notes/ML.html#chapter-23",
    "href": "notes/ML.html#chapter-23",
    "title": "Machine Learning",
    "section": "Chapter 23",
    "text": "Chapter 23\n\n23.3 Five principles of Prediction\n\n\n23.3.1 PREDICTIVE FEATURES DO NOT HAVE TO CAUSE THE OUTCOME\n\n\n23.3.2 CROSS-VALIDATION IS NOT ALWAYS A GOOD MEASURE OF PREDICTIVE POWER\n\n\n23.3.3 IT’S NOT ALWAYS BETTER TO BE MORE ACCURATE ON AVERAGE\n\n\n23.3.4 THERE CAN BE PRACTICAL VALUE IN INTERPRETING MODELS FOR PREDICTION\n\n\n23.3.5 IT CAN BE DIFFICULT TO APPLY PREDICTION TO POLICYMAKING"
  },
  {
    "objectID": "notes/ML.html#lecture-0-and-lecture-1",
    "href": "notes/ML.html#lecture-0-and-lecture-1",
    "title": "Machine Learning",
    "section": "",
    "text": "How is ML relevant for social science?\n\nbig data -&gt; human social behavior -&gt; social inquiry\nDigital Trace Data, digitalized content, image data \\(\\dots\\)\n\nCourse outline\n\nw1–4: supervised\nw5–7: unsupervised\nw8: ML for causal inference\n\ntwo types of supervised learning: prediction, inference\n\nprediction — an emphasis on \\(\\hat{Y}\\)\ninference — an emphasis on \\(\\hat{\\beta}\\)\n\nlimitations of standard linear paradigm (OLS)\n\nAssume linearity\nAssum additivity\nn &gt; p\nAssume we know which variables to include\n\nTo note — standardize oberservations first before regularization\nPros and cons (Ridge and Lasso)\npros:\n\nenables OLS\nimprove predictability\nimrpove interpretability\n\ncons:\n\nreduce inferential strength\ndoes not address non-linearity\n\nsplines — forces continuity at knots\nadditional constraint — linear at the boundaries"
  },
  {
    "objectID": "notes/ML.html#lecture-0-and-1",
    "href": "notes/ML.html#lecture-0-and-1",
    "title": "Machine Learning",
    "section": "",
    "text": "How is ML relevant for social science?\n\nbig data -&gt; human social behavior -&gt; social inquiry\nDigital Trace Data, digitalized content, image data \\(\\dots\\)\n\nCourse outline\n\nw1–4: supervised\nw5–7: unsupervised\nw8: ML for causal inference\n\ntwo types of supervised learning: prediction, inference\n\nprediction — an emphasis on \\(\\hat{Y}\\)\ninference — an emphasis on \\(\\hat{\\beta}\\)\n\nlimitations of standard linear paradigm (OLS)\n\nAssume linearity\nAssum additivity\nn &gt; p\nAssume we know which variables to include\n\nTo note — standardize oberservations first before regularization\nPros and cons (Ridge and Lasso)\npros:\n\nenables OLS\nimprove predictability\nimrpove interpretability\n\ncons:\n\nreduce inferential strength\ndoes not address non-linearity\n\nsplines — forces continuity at knots\nadditional constraint — linear at the boundaries"
  },
  {
    "objectID": "notes/ML.html#lecture-2-and-3",
    "href": "notes/ML.html#lecture-2-and-3",
    "title": "Machine Learning",
    "section": "",
    "text": "my question: What are the differences between interpretability and explanation\nmy question: Last month Duncan Watts mentioned two cultures: one is y hat culture emphasizing predictive performance, one is beta hat culture emphasizing causal effects. Both are important but neither alone is sufficient.\nmy question: how would you integrate prediction with interpretability or explanation, especially as a phd in analytical sociology. I understand that the explanation here could also mean mechanism-based explanation.\n(my question: I’m not sure if you were at Duncan Watts’ keynote last month in Norrköping, at IC2S2. If not, I’d just be curious how you think about integrating explanation and prediction，If you were there，then I’m also curious about his ideas on integrating explanation and prediction. How would you comment on his approach? And as a PhD in analytical sociology, how would you integrate explanation—especially mechanism-based explanation—with prediction? 废弃版)"
  },
  {
    "objectID": "notes/Hypergraph.html",
    "href": "notes/Hypergraph.html",
    "title": "Hypergraph",
    "section": "",
    "text": "A graph \\(G\\) is a set \\(X\\) of vertices together with a set \\(E\\) of edges. It is written as\n\\[\nG = (X, E)\n\\]"
  },
  {
    "objectID": "notes/Hypergraph.html#basic-definitions-and-concepts",
    "href": "notes/Hypergraph.html#basic-definitions-and-concepts",
    "title": "Hypergraph",
    "section": "",
    "text": "A graph \\(G\\) is a set \\(X\\) of vertices together with a set \\(E\\) of edges. It is written as\n\\[\nG = (X, E)\n\\]"
  }
]